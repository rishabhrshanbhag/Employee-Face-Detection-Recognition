{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imutils import face_utils\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Subtract, Input, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "cascPath = \"/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "def return_bbx(image):\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "    faces = faceCascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Adrien_Brody</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Alejandro_Toledo</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Alvaro_Uribe</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Amelie_Mauresmo</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>Vicente_Fox</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Vladimir_Putin</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>Wen_Jiabao</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659</th>\n",
       "      <td>Winona_Ryder</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>Yoriko_Kawaguchi</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  images\n",
       "20        Abdullah_Gul      19\n",
       "52        Adrien_Brody      12\n",
       "127   Alejandro_Toledo      39\n",
       "210       Alvaro_Uribe      35\n",
       "223    Amelie_Mauresmo      21\n",
       "...                ...     ...\n",
       "5541       Vicente_Fox      32\n",
       "5569    Vladimir_Putin      49\n",
       "5605        Wen_Jiabao      13\n",
       "5659      Winona_Ryder      24\n",
       "5704  Yoriko_Kawaguchi      14\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset_path = '/Users/abdulrehman/Desktop/SML Project/FacesInTheWild/'\n",
    "\n",
    "Celebs = pd.read_csv(Dataset_path+'lfw_allnames.csv')\n",
    "Celebs = Celebs[Celebs['images']>10]\n",
    "Celebs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = []\n",
    "X = []\n",
    "Y = []\n",
    "y_label = 0\n",
    "\n",
    "for _, [name,__] in Celebs.iterrows():\n",
    "    celeb_path = Dataset_path+'lfw-deepfunneled/'+name+'/'\n",
    "    \n",
    "    images_paths = get_files(celeb_path)\n",
    "    temp = []\n",
    "    for image_path in images_paths:\n",
    "        image = cv2.imread(celeb_path+image_path,1)\n",
    "        faces = return_bbx(image)\n",
    "        if len(faces) == 1:\n",
    "            if len(temp)>=10:\n",
    "                break\n",
    "            temp.append(len(X))\n",
    "            (x,y,w,h) = faces[0]\n",
    "            cropped = image[x:x+w, y:y+h]\n",
    "            dim = (224, 224)\n",
    "            resized = cv2.resize(cropped, dim, interpolation = cv2.INTER_AREA)\n",
    "            image = np.array(resized).astype(\"float32\")\n",
    "            X.append(image)\n",
    "            Y.append(y_label)\n",
    "    y_label+=1\n",
    "    cat_list.append(temp)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1423, 224, 224, 3) (1423,) (143,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.asarray(X)/255\n",
    "Y_data = np.array(Y)\n",
    "cat_list = np.asarray(cat_list)\n",
    "\n",
    "print(X_data.shape, Y_data.shape, cat_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# Counter(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Y_data\n",
    "n_classes = len(set(a))\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X&Y shape of training data : (1280, 224, 224, 3) and (1280,) (128,)\n",
      "X&Y shape of testing data : (143, 224, 224, 3) and (143,) (15,)\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "\n",
    "train_size = int(n_classes*train_split)\n",
    "test_size = n_classes-train_size\n",
    "\n",
    "train_files = train_size * 10\n",
    "\n",
    "X_train = X_data[:train_files]\n",
    "y_train = Y_data[:train_files]\n",
    "cat_train = cat_list[:train_size]\n",
    "\n",
    "#Validation Split\n",
    "X_test = X_data[train_files:]\n",
    "y_test = Y_data[train_files:]\n",
    "cat_test = cat_list[train_size:]\n",
    "\n",
    "print('X&Y shape of training data :',X_train.shape, 'and', y_train.shape, cat_train.shape)\n",
    "print('X&Y shape of testing data :' , X_test.shape, 'and', y_test.shape, cat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this function we generate batches for training and testing the data.\n",
    "This function imports data and creates data with 2 inputs for X and one output for each pair of outputs.\n",
    "\n",
    "get_batch takes data, categories, datasize, and batch_size as input,\n",
    "creates a numpy array Y with half the values 0 and half 1 and shuffles\n",
    "\n",
    "for every output that is 1 the input X contains images of two different categories\n",
    "for every outpyt that is 0 the input X contains images of the same categories\n",
    "'''\n",
    "\n",
    "def get_batch(data_x, data_cat, data_size, batch_size=64):\n",
    "\t\n",
    "\t#initializing the data for temporary use\n",
    "\ttemp_x = data_x\n",
    "\ttemp_cat_list = data_cat\n",
    "\n",
    "\tstart=0\n",
    "\tend=data_size\n",
    "\tbatch_x=[]\n",
    "\n",
    "\t# Initializing the Y output of size Batch_size, setting half the values to 0 and then shuffuling Y\n",
    "\tbatch_y = np.zeros(batch_size)\n",
    "\tbatch_y[int(batch_size/2):] = 1\n",
    "\tnp.random.shuffle(batch_y)\n",
    "\t\n",
    "\t# Class list is a list of random Categories.\n",
    "\t# Batch X is a list of 2 numpy arrays of shape (batch_size, 224, 224, 3)\n",
    "\tclass_list = np.random.randint(start, end, batch_size) \n",
    "\tbatch_x.append(np.zeros((batch_size, 224, 224, 3)))\n",
    "\tbatch_x.append(np.zeros((batch_size, 224, 224, 3)))\n",
    "\n",
    "\t# Traversing through all the X and Y.\n",
    "\t# Assigning same different images of the same subject to X if Y = 0 and different images of different subjects is Y = 1\n",
    "\tfor i in range(0, batch_size):\n",
    "\n",
    "\t\t'''\n",
    "\t\tFirst assign a random subjects image. as X[0] for ith position of the batch\n",
    "\t\tClass list is a list of random Categories, of size batch_size.\n",
    "\t\t- class_list[i] gets the random subject.\n",
    "\t\t- temp_cat_list[class_list[i]] gets the list of all the image positions from that randomply chosen category\n",
    "\t\t- np.random.choice on this list choses a ranom image from the list of all the image positions from the randomply chosen category\n",
    "\t\t- this randomly chosen image of a randomly chosen sunject is assigned to X[0] for the ith position of the batch.\n",
    "\t\t'''\n",
    "\n",
    "\t\tbatch_x[0][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]  \n",
    "\t\t'''\n",
    "\t\tNow the y value is checked.\n",
    "\t\tif the Y valus is 0 we assign a random image of the same categoruy to X[1]\n",
    "\t\tif the Y value is 1 we assign a random image of a random category to X[1]\n",
    "\t\t'''\n",
    "\t\tif batch_y[i]==0:\n",
    "\t\t\tbatch_x[1][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\ttemp_list = np.append(temp_cat_list[:class_list[i]], temp_cat_list[class_list[i]+1:])\n",
    "\t\t\ttemp_list = np.random.choice(temp_list)\n",
    "\t\t\tbatch_x[1][i] = temp_x[np.random.choice(temp_list)]            \n",
    "\t\t\t\n",
    "\treturn(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this function we create a siamese network with 2 vgg16 networks working in parallel.\n",
    "we use a convolutional model to find feature maps of images.\n",
    "each convolutional neural netwirk takes an inage and outputs the feature maps of these images\n",
    "Theses image feature maps are then compared using a subtraction layer.\n",
    "The subtraction layer then finds the distance between the feature maps of the images\n",
    "this disctance is then taken as input to the a Fully connected layer which predicts if the images belong to the same subject.\n",
    "In this function we create the model.\n",
    "'''\n",
    "\n",
    "def get_model(input_shape):\n",
    "\n",
    "\tleft_input = Input(input_shape)\n",
    "\tright_input = Input(input_shape)\n",
    "\n",
    "\tleft = Sequential()\n",
    "\tleft.add(left_input)\n",
    "\tleft.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Flatten())\n",
    "\tleft.add(Dense(1028, activation='relu', kernel_regularizer=l2(1e-2)))\n",
    "\n",
    "\tright = Sequential()\n",
    "\tright.add(right_input)\n",
    "\tright.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Flatten())\n",
    "\tright.add(Dense(1028, activation='relu'))\n",
    "\n",
    "\tsubtracted = Subtract()([left.output,right.output])\n",
    "\tsubtracted = Dense(512, activation='relu')(subtracted)\n",
    "\tsubtracted = Dense(128, activation='relu')(subtracted)\n",
    "\tout = Dense(2, activation='softmax')(subtracted)\n",
    "\n",
    "\tmodel = Model(inputs = [left.input, right.input], outputs = out)\n",
    "\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "\treturn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vgg(input_shape):\n",
    "\n",
    "\tvgg_left = VGG16(weights = 'imagenet',include_top = False, input_shape = input_shape)\n",
    "\n",
    "\tfor layer in vgg_left.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'left_'+layer.name\n",
    "\t\t\n",
    "\tleft = [layer.output for layer in vgg_left.layers][-5]\n",
    "\n",
    "\tleft = Flatten()(left)\n",
    "\t# left = Dropout(0.5)(left)\n",
    "\tleft = Dense(4096, kernel_regularizer=l2(1e-2))(left)\n",
    "\tleft = BatchNormalization()(left)\n",
    "\tleft = Activation('sigmoid')(left)\n",
    "\n",
    "\n",
    "\tvgg_right = VGG16(weights = 'imagenet',include_top = False, input_shape = input_shape)\n",
    "\n",
    "\tfor layer in vgg_right.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'right_'+layer.name\n",
    "\n",
    "\tright = [layer.output for layer in vgg_right.layers][-5]\n",
    "\n",
    "\tright = Flatten()(right)\n",
    "\t# right = Dropout(0.5)(right)\n",
    "\tright = Dense(4096, kernel_regularizer=l2(1e-2))(right)\n",
    "\tright = BatchNormalization()(right)\n",
    "\tright = Activation('sigmoid')(right)\n",
    "\n",
    "\n",
    "\tsubtracted = Subtract()([left,right])\n",
    "\tsubtracted = Dense(1024, activation='sigmoid')(subtracted)\n",
    "\tsubtracted = Dense(512, activation='sigmoid')(subtracted)\n",
    "\tout = Dense(2, activation='softmax')(subtracted)\n",
    "\n",
    "\tmodel = Model(inputs = [vgg_left.input,vgg_right.input], outputs = out)\n",
    "\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "\treturn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "def get_vgg_face(input_shape):\n",
    "    \n",
    "\tlayer_name = 'fc7/relu'\n",
    "\n",
    "\tvgg_left = VGGFace()\n",
    "\n",
    "\tfor layer in vgg_left.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'left_'+layer.name\n",
    "\t\t\n",
    "\tleft = vgg_left.get_layer('left_'+layer_name).output\n",
    "\n",
    "# \tleft = Flatten()(left)\n",
    "\t# left = Dropout(0.5)(left)\n",
    "\tleft = Dense(128, kernel_regularizer=l2(1e-2))(left)\n",
    "# \tleft = BatchNormalization()(left)\n",
    "\tleft = Activation('relu')(left)\n",
    "\n",
    "\n",
    "\tvgg_right = VGGFace()\n",
    "\n",
    "\tfor layer in vgg_right.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'right_'+layer.name\n",
    "\n",
    "\tright = vgg_right.get_layer('right_'+layer_name).output\n",
    "\n",
    "# \tright = Flatten()(right)\n",
    "# \t# right = Dropout(0.5)(right)\n",
    "\tright = Dense(128, kernel_regularizer=l2(1e-2))(right)\n",
    "# \tright = BatchNormalization()(right)\n",
    "\tright = Activation('relu')(right)\n",
    "\n",
    "\n",
    "\tsubtracted = Subtract()([left,right])\n",
    "# \tsubtracted = Dense(1024, activation='relu')(subtracted)\n",
    "\tsubtracted = Dense(64, activation='relu')(subtracted)\n",
    "\tout = Dense(1, activation='sigmoid')(subtracted)\n",
    "\n",
    "\tmodel = Model(inputs = [vgg_left.input,vgg_right.input], outputs = out)\n",
    "\n",
    "\tmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "\treturn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the function one shot learning we use the trained model to test some validation data and compute the accuracy\n",
    "'''\n",
    "\n",
    "def one_shot_learning(model, n_way, n_val):\n",
    "\t\n",
    "\t#initializing the data for temporary use\n",
    "\ttemp_x = X_test\n",
    "\ttemp_cat_list = cat_test\n",
    "\n",
    "\tbatch_x=[]\n",
    "\tx_0_choice=[]\n",
    "\tn_correct = 0\n",
    "\t\n",
    "\t# Class list is a list of random Categories from the test data of n_val length.\n",
    "\tclass_list = np.random.randint(train_size+1, n_classes-1, n_val)\n",
    "\n",
    "\tfor i in class_list:  \n",
    "\t\t# j = class_list[i] gets the random subject. \n",
    "\t\t# J is a randomly chosen image from a randomly chosen subject from the test data.\n",
    "\t\tj = np.random.choice(cat_list[i])\n",
    "\n",
    "\t\t# temp is a list of 2 numpy arrays of shape (n_way, 100, 100, 3)\n",
    "\t\ttemp=[]\n",
    "\t\ttemp.append(np.zeros((n_way, 224, 224, 3)))\n",
    "\t\ttemp.append(np.zeros((n_way, 224, 224, 3)))\n",
    "\n",
    "\t\t'''\n",
    "\t\tnow in the for loop we are going to create n_way image pairs.\n",
    "\t\tthe 1st image in all the pairs will be a random image j of the randomly chosen subject\n",
    "\t\tthe second image for the first pair at position will belong to the the same subject as the subject of j\n",
    "\t\tthe second image for all the other pairs will be from random subjects.\n",
    "\t\t'''\n",
    "\t\tfor k in range(0, n_way):\n",
    "\t\t\t# Assigning the first pair of each image with the same image j of a random subject\n",
    "\t\t\ttemp[0][k] = X_data[j]\n",
    "\t\t\t\n",
    "\t\t\t# Assigning the same subjects image as the second image of tbe first pair\n",
    "\t\t\tif k==0:\n",
    "\t\t\t\ttemp[1][k] = X_data[np.random.choice(cat_list[i])]\n",
    "\n",
    "\t\t\t# Assigning the different subjects image as the second image of tbe all pairs except the first pair.\n",
    "\t\t\telse:\n",
    "\t\t\t\ttemp_list = np.append(cat_list[:i], cat_list[i+1:])\n",
    "\t\t\t\ttemp_list = np.random.choice(temp_list)\n",
    "\t\t\t\ttemp[1][k] = X_data[np.random.choice(temp_list)]\n",
    "\n",
    "\t\tresult = model.predict(temp)\n",
    "# \t\tresult = np.argmax(result, axis=1).tolist()\n",
    "# \t\ty_true = [1 for ]\n",
    "# \t\taccuracy = accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
    "\t\t\n",
    "# \t\treturn N\n",
    "\t\tresult = result.flatten().tolist()\n",
    "\t\tresult_index = result.index(min(result))\n",
    "\t\tif result_index == 0:\n",
    "\t\t\tn_correct = n_correct + 1\n",
    "\tprint(n_correct, \" correctly classified among \", n_val)\n",
    "\taccuracy = (n_correct*100)/n_val\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_with_batch(model, X_train,cat_train,train_size,epochs,enc,n_way = 20,n_val = 64,batch_size = 64):\n",
    "\tloss_list=[]\n",
    "\taccuracy_list=[]\n",
    "\tfor epoch in range(1,epochs):\n",
    "\t\tbatch_x, batch_y = get_batch(X_train, cat_train, train_size, batch_size)\n",
    "# \t\tbatch_y = enc.fit_transform(batch_y.reshape(-1,1))\n",
    "\t\tloss = model.train_on_batch(batch_x, batch_y)\n",
    "\t\tloss_list.append((epoch,loss))\n",
    "\t\tprint('Epoch:', epoch, ', Loss:',loss)\n",
    "\t\tif epoch%100 == 0:\n",
    "\t\t\tprint(\"=============================================\")\n",
    "\t\t\taccuracy = one_shot_learning(model, n_way, n_val)\n",
    "\t\t\taccuracy_list.append((epoch, accuracy))\n",
    "\t\t\tprint('Accuracy as of', epoch, 'epochs:', accuracy)\n",
    "\t\t\tprint(\"=============================================\")\n",
    "\t\t\tif(accuracy>99):\n",
    "\t\t\t\tprint(\"Achieved more than 90% Accuracy\")\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: [5.659049987792969, 0.51171875]\n",
      "Epoch: 2 , Loss: [5.626762390136719, 0.5078125]\n",
      "Epoch: 3 , Loss: [5.58784818649292, 0.5078125]\n",
      "Epoch: 4 , Loss: [5.553286552429199, 0.5234375]\n",
      "Epoch: 5 , Loss: [5.525815010070801, 0.44140625]\n",
      "Epoch: 6 , Loss: [5.475176811218262, 0.5390625]\n",
      "Epoch: 7 , Loss: [5.449907302856445, 0.49609375]\n",
      "Epoch: 8 , Loss: [5.409162521362305, 0.55078125]\n",
      "Epoch: 9 , Loss: [5.383829116821289, 0.5]\n",
      "Epoch: 10 , Loss: [5.344635963439941, 0.51953125]\n",
      "Epoch: 11 , Loss: [5.312382698059082, 0.53515625]\n",
      "Epoch: 12 , Loss: [5.280072212219238, 0.55078125]\n",
      "Epoch: 13 , Loss: [5.24574613571167, 0.53515625]\n",
      "Epoch: 14 , Loss: [5.209768295288086, 0.55078125]\n",
      "Epoch: 15 , Loss: [5.179605484008789, 0.546875]\n",
      "Epoch: 16 , Loss: [5.149595260620117, 0.4921875]\n",
      "Epoch: 17 , Loss: [5.120455741882324, 0.4765625]\n",
      "Epoch: 18 , Loss: [5.076554298400879, 0.5859375]\n",
      "Epoch: 19 , Loss: [5.057098388671875, 0.5078125]\n",
      "Epoch: 20 , Loss: [5.0156145095825195, 0.52734375]\n",
      "Epoch: 21 , Loss: [4.979024887084961, 0.6015625]\n",
      "Epoch: 22 , Loss: [4.940582752227783, 0.609375]\n",
      "Epoch: 23 , Loss: [4.921396732330322, 0.55078125]\n",
      "Epoch: 24 , Loss: [4.887701988220215, 0.56640625]\n",
      "Epoch: 25 , Loss: [4.861945152282715, 0.53125]\n",
      "Epoch: 26 , Loss: [4.833943843841553, 0.52734375]\n",
      "Epoch: 27 , Loss: [4.805339813232422, 0.4765625]\n",
      "Epoch: 28 , Loss: [4.768019199371338, 0.55859375]\n",
      "Epoch: 29 , Loss: [4.741352081298828, 0.50390625]\n",
      "Epoch: 30 , Loss: [4.702829360961914, 0.60546875]\n",
      "Epoch: 31 , Loss: [4.674631118774414, 0.5625]\n",
      "Epoch: 32 , Loss: [4.647066116333008, 0.57421875]\n",
      "Epoch: 33 , Loss: [4.617647171020508, 0.5390625]\n",
      "Epoch: 34 , Loss: [4.58491849899292, 0.55078125]\n",
      "Epoch: 35 , Loss: [4.55933141708374, 0.546875]\n",
      "Epoch: 36 , Loss: [4.533953666687012, 0.54296875]\n",
      "Epoch: 37 , Loss: [4.489354133605957, 0.60546875]\n",
      "Epoch: 38 , Loss: [4.464724540710449, 0.5859375]\n",
      "Epoch: 39 , Loss: [4.43766450881958, 0.578125]\n",
      "Epoch: 40 , Loss: [4.4107279777526855, 0.58203125]\n",
      "Epoch: 41 , Loss: [4.385105133056641, 0.5859375]\n",
      "Epoch: 42 , Loss: [4.355149269104004, 0.6015625]\n",
      "Epoch: 43 , Loss: [4.32635498046875, 0.59375]\n",
      "Epoch: 44 , Loss: [4.29625940322876, 0.63671875]\n",
      "Epoch: 45 , Loss: [4.269275188446045, 0.62890625]\n",
      "Epoch: 46 , Loss: [4.243121147155762, 0.6171875]\n",
      "Epoch: 47 , Loss: [4.215094089508057, 0.640625]\n",
      "Epoch: 48 , Loss: [4.190891742706299, 0.61328125]\n",
      "Epoch: 49 , Loss: [4.160154342651367, 0.64453125]\n",
      "Epoch: 50 , Loss: [4.133543491363525, 0.6484375]\n",
      "Epoch: 51 , Loss: [4.110862731933594, 0.625]\n",
      "Epoch: 52 , Loss: [4.088927268981934, 0.5859375]\n",
      "Epoch: 53 , Loss: [4.06055212020874, 0.59765625]\n",
      "Epoch: 54 , Loss: [4.0264058113098145, 0.66796875]\n",
      "Epoch: 55 , Loss: [4.002871036529541, 0.70703125]\n",
      "Epoch: 56 , Loss: [3.977696180343628, 0.63671875]\n",
      "Epoch: 57 , Loss: [3.956200361251831, 0.6171875]\n",
      "Epoch: 58 , Loss: [3.930243968963623, 0.65234375]\n",
      "Epoch: 59 , Loss: [3.898858070373535, 0.66796875]\n",
      "Epoch: 60 , Loss: [3.879889488220215, 0.64453125]\n",
      "Epoch: 61 , Loss: [3.8521430492401123, 0.6796875]\n",
      "Epoch: 62 , Loss: [3.8300137519836426, 0.6875]\n",
      "Epoch: 63 , Loss: [3.8091464042663574, 0.66796875]\n",
      "Epoch: 64 , Loss: [3.7843918800354004, 0.703125]\n",
      "Epoch: 65 , Loss: [3.754897117614746, 0.67578125]\n",
      "Epoch: 66 , Loss: [3.7288055419921875, 0.66796875]\n",
      "Epoch: 67 , Loss: [3.720299482345581, 0.59375]\n",
      "Epoch: 68 , Loss: [3.696652412414551, 0.61328125]\n",
      "Epoch: 69 , Loss: [3.6596360206604004, 0.69921875]\n",
      "Epoch: 70 , Loss: [3.634620428085327, 0.6796875]\n",
      "Epoch: 71 , Loss: [3.6197612285614014, 0.65625]\n",
      "Epoch: 72 , Loss: [3.593395709991455, 0.66015625]\n",
      "Epoch: 73 , Loss: [3.5703048706054688, 0.6796875]\n",
      "Epoch: 74 , Loss: [3.5508675575256348, 0.67578125]\n",
      "Epoch: 75 , Loss: [3.523955821990967, 0.73046875]\n",
      "Epoch: 76 , Loss: [3.5069780349731445, 0.66796875]\n",
      "Epoch: 77 , Loss: [3.4830031394958496, 0.66796875]\n",
      "Epoch: 78 , Loss: [3.461268424987793, 0.65625]\n",
      "Epoch: 79 , Loss: [3.4453015327453613, 0.65234375]\n",
      "Epoch: 80 , Loss: [3.4107840061187744, 0.75390625]\n",
      "Epoch: 81 , Loss: [3.3914082050323486, 0.72265625]\n",
      "Epoch: 82 , Loss: [3.367520332336426, 0.73828125]\n",
      "Epoch: 83 , Loss: [3.35072660446167, 0.75]\n",
      "Epoch: 84 , Loss: [3.3275203704833984, 0.7109375]\n",
      "Epoch: 85 , Loss: [3.3091118335723877, 0.73828125]\n",
      "Epoch: 86 , Loss: [3.284418821334839, 0.74609375]\n",
      "Epoch: 87 , Loss: [3.262251853942871, 0.7578125]\n",
      "Epoch: 88 , Loss: [3.244943141937256, 0.75]\n",
      "Epoch: 89 , Loss: [3.220594882965088, 0.76953125]\n",
      "Epoch: 90 , Loss: [3.202725887298584, 0.75390625]\n",
      "Epoch: 91 , Loss: [3.184487819671631, 0.7265625]\n",
      "Epoch: 92 , Loss: [3.1544089317321777, 0.8203125]\n",
      "Epoch: 93 , Loss: [3.1382055282592773, 0.7734375]\n",
      "Epoch: 94 , Loss: [3.1267151832580566, 0.765625]\n",
      "Epoch: 95 , Loss: [3.0985546112060547, 0.7890625]\n",
      "Epoch: 96 , Loss: [3.0954601764678955, 0.6953125]\n",
      "Epoch: 97 , Loss: [3.06354022026062, 0.74609375]\n",
      "Epoch: 98 , Loss: [3.0403552055358887, 0.78125]\n",
      "Epoch: 99 , Loss: [3.0192482471466064, 0.8046875]\n",
      "Epoch: 100 , Loss: [3.0038647651672363, 0.75390625]\n",
      "=============================================\n",
      "13  correctly classified among  64\n",
      "Accuracy as of 100 epochs: 20.3125\n",
      "=============================================\n",
      "Epoch: 101 , Loss: [2.9853219985961914, 0.76171875]\n",
      "Epoch: 102 , Loss: [2.9653689861297607, 0.765625]\n",
      "Epoch: 103 , Loss: [2.9535491466522217, 0.734375]\n",
      "Epoch: 104 , Loss: [2.9236271381378174, 0.8046875]\n",
      "Epoch: 105 , Loss: [2.919215202331543, 0.73828125]\n",
      "Epoch: 106 , Loss: [2.8965377807617188, 0.76953125]\n",
      "Epoch: 107 , Loss: [2.869670867919922, 0.796875]\n",
      "Epoch: 108 , Loss: [2.8531908988952637, 0.76953125]\n",
      "Epoch: 109 , Loss: [2.8401951789855957, 0.75]\n",
      "Epoch: 110 , Loss: [2.8190102577209473, 0.8359375]\n",
      "Epoch: 111 , Loss: [2.799691677093506, 0.7734375]\n",
      "Epoch: 112 , Loss: [2.7858152389526367, 0.7734375]\n",
      "Epoch: 113 , Loss: [2.764188289642334, 0.828125]\n",
      "Epoch: 114 , Loss: [2.748537540435791, 0.8046875]\n",
      "Epoch: 115 , Loss: [2.736361503601074, 0.7421875]\n",
      "Epoch: 116 , Loss: [2.707693099975586, 0.79296875]\n",
      "Epoch: 117 , Loss: [2.705115795135498, 0.7421875]\n",
      "Epoch: 118 , Loss: [2.6832661628723145, 0.81640625]\n",
      "Epoch: 119 , Loss: [2.66560435295105, 0.7578125]\n",
      "Epoch: 120 , Loss: [2.6466145515441895, 0.7890625]\n",
      "Epoch: 121 , Loss: [2.61940336227417, 0.859375]\n",
      "Epoch: 122 , Loss: [2.616023063659668, 0.78515625]\n",
      "Epoch: 123 , Loss: [2.6026978492736816, 0.81640625]\n",
      "Epoch: 124 , Loss: [2.575896739959717, 0.8359375]\n",
      "Epoch: 125 , Loss: [2.5785622596740723, 0.79296875]\n",
      "Epoch: 126 , Loss: [2.542748212814331, 0.84375]\n",
      "Epoch: 127 , Loss: [2.523383617401123, 0.8125]\n",
      "Epoch: 128 , Loss: [2.513796806335449, 0.82421875]\n",
      "Epoch: 129 , Loss: [2.499941110610962, 0.828125]\n",
      "Epoch: 130 , Loss: [2.489048480987549, 0.796875]\n",
      "Epoch: 131 , Loss: [2.467374086380005, 0.84375]\n",
      "Epoch: 132 , Loss: [2.464144706726074, 0.828125]\n",
      "Epoch: 133 , Loss: [2.4453179836273193, 0.79296875]\n",
      "Epoch: 134 , Loss: [2.4222419261932373, 0.83984375]\n",
      "Epoch: 135 , Loss: [2.410320520401001, 0.8125]\n",
      "Epoch: 136 , Loss: [2.3938815593719482, 0.8125]\n",
      "Epoch: 137 , Loss: [2.371506690979004, 0.84375]\n",
      "Epoch: 138 , Loss: [2.3541600704193115, 0.8125]\n",
      "Epoch: 139 , Loss: [2.3460285663604736, 0.86328125]\n",
      "Epoch: 140 , Loss: [2.337137222290039, 0.8125]\n",
      "Epoch: 141 , Loss: [2.3144469261169434, 0.8359375]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "n_way = 20\n",
    "n_val = 64\n",
    "batch_size = 256\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "model = get_vgg_face((224,224,3))\n",
    "\n",
    "model = Train_with_batch(model, X_train,cat_train,train_size,epochs,enc,n_way = 20,n_val = 64,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "20/20 [==============================] - 346s 17s/step - loss: 5.3880 - accuracy: 0.5430\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - 332s 17s/step - loss: 4.8286 - accuracy: 0.6750\n",
      "Epoch 3/1000\n",
      " 7/20 [=========>....................] - ETA: 3:11 - loss: 4.4802 - accuracy: 0.7567"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0937fe6dd402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# batch_y = enc.fit_transform(batch_y.reshape(-1,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "n_way = 20\n",
    "n_val = 64\n",
    "batch_size = 1280\n",
    "\n",
    "# enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "model = get_vgg_face((224,224,3))\n",
    "\n",
    "batch_x, batch_y = get_batch(X_train, cat_train, train_size, batch_size)\n",
    "# batch_y = enc.fit_transform(batch_y.reshape(-1,1))\n",
    "\n",
    "history = model.fit(batch_x, batch_y,batch_size = 64,epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Face-Detection] *",
   "language": "python",
   "name": "conda-env-Face-Detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
