{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imutils import face_utils\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Subtract, Input, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "cascPath = \"/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "def return_bbx(image):\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "    faces = faceCascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Adrien_Brody</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Alejandro_Toledo</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Alvaro_Uribe</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Amelie_Mauresmo</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>Vicente_Fox</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Vladimir_Putin</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>Wen_Jiabao</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659</th>\n",
       "      <td>Winona_Ryder</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>Yoriko_Kawaguchi</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  images\n",
       "20        Abdullah_Gul      19\n",
       "52        Adrien_Brody      12\n",
       "127   Alejandro_Toledo      39\n",
       "210       Alvaro_Uribe      35\n",
       "223    Amelie_Mauresmo      21\n",
       "...                ...     ...\n",
       "5541       Vicente_Fox      32\n",
       "5569    Vladimir_Putin      49\n",
       "5605        Wen_Jiabao      13\n",
       "5659      Winona_Ryder      24\n",
       "5704  Yoriko_Kawaguchi      14\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset_path = '/Users/abdulrehman/Desktop/SML Project/FacesInTheWild/'\n",
    "\n",
    "Celebs = pd.read_csv(Dataset_path+'lfw_allnames.csv')\n",
    "Celebs = Celebs[Celebs['images']>10]\n",
    "Celebs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = []\n",
    "X = []\n",
    "Y = []\n",
    "y_label = 0\n",
    "\n",
    "for _, [name,__] in Celebs.iterrows():\n",
    "    celeb_path = Dataset_path+'lfw-deepfunneled/'+name+'/'\n",
    "    \n",
    "    images_paths = get_files(celeb_path)\n",
    "    temp = []\n",
    "    for image_path in images_paths:\n",
    "        image = cv2.imread(celeb_path+image_path,1)\n",
    "        faces = return_bbx(image)\n",
    "        if len(faces) == 1:\n",
    "            if len(temp)>=10:\n",
    "                break\n",
    "            temp.append(len(X))\n",
    "            (x,y,w,h) = faces[0]\n",
    "            cropped = image[x:x+w, y:y+h]\n",
    "            dim = (224, 224)\n",
    "            resized = cv2.resize(cropped, dim, interpolation = cv2.INTER_AREA)\n",
    "            image = np.array(resized).astype(\"float32\")\n",
    "            X.append(image)\n",
    "            Y.append(y_label)\n",
    "    y_label+=1\n",
    "    cat_list.append(temp)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1423, 224, 224, 3) (1423,) (143,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.asarray(X)/255\n",
    "Y_data = np.array(Y)\n",
    "cat_list = np.asarray(cat_list)\n",
    "\n",
    "print(X_data.shape, Y_data.shape, cat_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# Counter(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Y_data\n",
    "n_classes = len(set(a))\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X&Y shape of training data : (1280, 224, 224, 3) and (1280,) (128,)\n",
      "X&Y shape of testing data : (143, 224, 224, 3) and (143,) (15,)\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "\n",
    "train_size = int(n_classes*train_split)\n",
    "test_size = n_classes-train_size\n",
    "\n",
    "train_files = train_size * 10\n",
    "\n",
    "X_train = X_data[:train_files]\n",
    "y_train = Y_data[:train_files]\n",
    "cat_train = cat_list[:train_size]\n",
    "\n",
    "#Validation Split\n",
    "X_test = X_data[train_files:]\n",
    "y_test = Y_data[train_files:]\n",
    "cat_test = cat_list[train_size:]\n",
    "\n",
    "print('X&Y shape of training data :',X_train.shape, 'and', y_train.shape, cat_train.shape)\n",
    "print('X&Y shape of testing data :' , X_test.shape, 'and', y_test.shape, cat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this function we generate batches for training and testing the data.\n",
    "This function imports data and creates data with 2 inputs for X and one output for each pair of outputs.\n",
    "\n",
    "get_batch takes data, categories, datasize, and batch_size as input,\n",
    "creates a numpy array Y with half the values 0 and half 1 and shuffles\n",
    "\n",
    "for every output that is 1 the input X contains images of two different categories\n",
    "for every outpyt that is 0 the input X contains images of the same categories\n",
    "'''\n",
    "\n",
    "def get_batch(data_x, data_cat, data_size, batch_size=64):\n",
    "\t\n",
    "\t#initializing the data for temporary use\n",
    "\ttemp_x = data_x\n",
    "\ttemp_cat_list = data_cat\n",
    "\n",
    "\tstart=0\n",
    "\tend=data_size\n",
    "\tbatch_x=[]\n",
    "\n",
    "\t# Initializing the Y output of size Batch_size, setting half the values to 0 and then shuffuling Y\n",
    "\tbatch_y = np.zeros(batch_size)\n",
    "\tbatch_y[int(batch_size/2):] = 1\n",
    "\tnp.random.shuffle(batch_y)\n",
    "\t\n",
    "\t# Class list is a list of random Categories.\n",
    "\t# Batch X is a list of 2 numpy arrays of shape (batch_size, 224, 224, 3)\n",
    "\tclass_list = np.random.randint(start, end, batch_size) \n",
    "\tbatch_x.append(np.zeros((batch_size, 224, 224, 3)))\n",
    "\tbatch_x.append(np.zeros((batch_size, 224, 224, 3)))\n",
    "\n",
    "\t# Traversing through all the X and Y.\n",
    "\t# Assigning same different images of the same subject to X if Y = 0 and different images of different subjects is Y = 1\n",
    "\tfor i in range(0, batch_size):\n",
    "\n",
    "\t\t'''\n",
    "\t\tFirst assign a random subjects image. as X[0] for ith position of the batch\n",
    "\t\tClass list is a list of random Categories, of size batch_size.\n",
    "\t\t- class_list[i] gets the random subject.\n",
    "\t\t- temp_cat_list[class_list[i]] gets the list of all the image positions from that randomply chosen category\n",
    "\t\t- np.random.choice on this list choses a ranom image from the list of all the image positions from the randomply chosen category\n",
    "\t\t- this randomly chosen image of a randomly chosen sunject is assigned to X[0] for the ith position of the batch.\n",
    "\t\t'''\n",
    "\n",
    "\t\tbatch_x[0][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]  \n",
    "\t\t'''\n",
    "\t\tNow the y value is checked.\n",
    "\t\tif the Y valus is 0 we assign a random image of the same categoruy to X[1]\n",
    "\t\tif the Y value is 1 we assign a random image of a random category to X[1]\n",
    "\t\t'''\n",
    "\t\tif batch_y[i]==0:\n",
    "\t\t\tbatch_x[1][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\ttemp_list = np.append(temp_cat_list[:class_list[i]], temp_cat_list[class_list[i]+1:])\n",
    "\t\t\ttemp_list = np.random.choice(temp_list)\n",
    "\t\t\tbatch_x[1][i] = temp_x[np.random.choice(temp_list)]            \n",
    "\t\t\t\n",
    "\treturn(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this function we create a siamese network with 2 vgg16 networks working in parallel.\n",
    "we use a convolutional model to find feature maps of images.\n",
    "each convolutional neural netwirk takes an inage and outputs the feature maps of these images\n",
    "Theses image feature maps are then compared using a subtraction layer.\n",
    "The subtraction layer then finds the distance between the feature maps of the images\n",
    "this disctance is then taken as input to the a Fully connected layer which predicts if the images belong to the same subject.\n",
    "In this function we create the model.\n",
    "'''\n",
    "\n",
    "def get_model(input_shape):\n",
    "\n",
    "\tleft_input = Input(input_shape)\n",
    "\tright_input = Input(input_shape)\n",
    "\n",
    "\tleft = Sequential()\n",
    "\tleft.add(left_input)\n",
    "\tleft.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\tleft.add(MaxPooling2D(2,2))\n",
    "\tleft.add(Flatten())\n",
    "\tleft.add(Dense(1028, activation='relu', kernel_regularizer=l2(1e-2)))\n",
    "\n",
    "\tright = Sequential()\n",
    "\tright.add(right_input)\n",
    "\tright.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\tright.add(MaxPooling2D(2,2))\n",
    "\tright.add(Flatten())\n",
    "\tright.add(Dense(1028, activation='relu'))\n",
    "\n",
    "\tsubtracted = Subtract()([left.output,right.output])\n",
    "\tsubtracted = Dense(512, activation='relu')(subtracted)\n",
    "\tsubtracted = Dense(128, activation='relu')(subtracted)\n",
    "\tout = Dense(2, activation='softmax')(subtracted)\n",
    "\n",
    "\tmodel = Model(inputs = [left.input, right.input], outputs = out)\n",
    "\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "\treturn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vgg(input_shape):\n",
    "\n",
    "\tvgg_left = VGG16(weights = 'imagenet',include_top = False, input_shape = input_shape)\n",
    "\n",
    "\tfor layer in vgg_left.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'left_'+layer.name\n",
    "\t\t\n",
    "\tleft = [layer.output for layer in vgg_left.layers][-5]\n",
    "\n",
    "\tleft = Flatten()(left)\n",
    "\t# left = Dropout(0.5)(left)\n",
    "\tleft = Dense(4096, kernel_regularizer=l2(1e-2))(left)\n",
    "\tleft = BatchNormalization()(left)\n",
    "\tleft = Activation('sigmoid')(left)\n",
    "\n",
    "\n",
    "\tvgg_right = VGG16(weights = 'imagenet',include_top = False, input_shape = input_shape)\n",
    "\n",
    "\tfor layer in vgg_right.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'right_'+layer.name\n",
    "\n",
    "\tright = [layer.output for layer in vgg_right.layers][-5]\n",
    "\n",
    "\tright = Flatten()(right)\n",
    "\t# right = Dropout(0.5)(right)\n",
    "\tright = Dense(4096, kernel_regularizer=l2(1e-2))(right)\n",
    "\tright = BatchNormalization()(right)\n",
    "\tright = Activation('sigmoid')(right)\n",
    "\n",
    "\n",
    "\tsubtracted = Subtract()([left,right])\n",
    "\tsubtracted = Dense(1024, activation='sigmoid')(subtracted)\n",
    "\tsubtracted = Dense(512, activation='sigmoid')(subtracted)\n",
    "\tout = Dense(2, activation='softmax')(subtracted)\n",
    "\n",
    "\tmodel = Model(inputs = [vgg_left.input,vgg_right.input], outputs = out)\n",
    "\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "\treturn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "def get_vgg_face(input_shape):\n",
    "    \n",
    "\tlayer_name = 'fc7/relu'\n",
    "\n",
    "\tvgg_left = VGGFace()\n",
    "\n",
    "\tfor layer in vgg_left.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'left_'+layer.name\n",
    "\t\t\n",
    "\tleft = vgg_left.get_layer('left_'+layer_name).output\n",
    "\n",
    "# \tleft = Flatten()(left)\n",
    "\t# left = Dropout(0.5)(left)\n",
    "\tleft = Dense(128, kernel_regularizer=l2(1e-2))(left)\n",
    "# \tleft = BatchNormalization()(left)\n",
    "\tleft = Activation('relu')(left)\n",
    "\n",
    "\n",
    "\tvgg_right = VGGFace()\n",
    "\n",
    "\tfor layer in vgg_right.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t\tlayer._name = 'right_'+layer.name\n",
    "\n",
    "\tright = vgg_right.get_layer('right_'+layer_name).output\n",
    "\n",
    "# \tright = Flatten()(right)\n",
    "# \t# right = Dropout(0.5)(right)\n",
    "\tright = Dense(128, kernel_regularizer=l2(1e-2))(right)\n",
    "# \tright = BatchNormalization()(right)\n",
    "\tright = Activation('relu')(right)\n",
    "\n",
    "\n",
    "\tsubtracted = Subtract()([left,right])\n",
    "# \tsubtracted = Dense(1024, activation='relu')(subtracted)\n",
    "\tsubtracted = Dense(64, activation='relu')(subtracted)\n",
    "\tout = Dense(1, activation='sigmoid')(subtracted)\n",
    "\n",
    "\tmodel = Model(inputs = [vgg_left.input,vgg_right.input], outputs = out)\n",
    "\n",
    "\tmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "\treturn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the function one shot learning we use the trained model to test some validation data and compute the accuracy\n",
    "'''\n",
    "\n",
    "def one_shot_learning(model, n_way, n_val):\n",
    "\t\n",
    "\t#initializing the data for temporary use\n",
    "\ttemp_x = X_test\n",
    "\ttemp_cat_list = cat_test\n",
    "\n",
    "\tbatch_x=[]\n",
    "\tx_0_choice=[]\n",
    "\tn_correct = 0\n",
    "\t\n",
    "\t# Class list is a list of random Categories from the test data of n_val length.\n",
    "\tclass_list = np.random.randint(train_size+1, n_classes-1, n_val)\n",
    "\n",
    "\tfor i in class_list:  \n",
    "\t\t# j = class_list[i] gets the random subject. \n",
    "\t\t# J is a randomly chosen image from a randomly chosen subject from the test data.\n",
    "\t\tj = np.random.choice(cat_list[i])\n",
    "\n",
    "\t\t# temp is a list of 2 numpy arrays of shape (n_way, 100, 100, 3)\n",
    "\t\ttemp=[]\n",
    "\t\ttemp.append(np.zeros((n_way, 224, 224, 3)))\n",
    "\t\ttemp.append(np.zeros((n_way, 224, 224, 3)))\n",
    "\n",
    "\t\t'''\n",
    "\t\tnow in the for loop we are going to create n_way image pairs.\n",
    "\t\tthe 1st image in all the pairs will be a random image j of the randomly chosen subject\n",
    "\t\tthe second image for the first pair at position will belong to the the same subject as the subject of j\n",
    "\t\tthe second image for all the other pairs will be from random subjects.\n",
    "\t\t'''\n",
    "\t\tfor k in range(0, n_way):\n",
    "\t\t\t# Assigning the first pair of each image with the same image j of a random subject\n",
    "\t\t\ttemp[0][k] = X_data[j]\n",
    "\t\t\t\n",
    "\t\t\t# Assigning the same subjects image as the second image of tbe first pair\n",
    "\t\t\tif k==0:\n",
    "\t\t\t\ttemp[1][k] = X_data[np.random.choice(cat_list[i])]\n",
    "\n",
    "\t\t\t# Assigning the different subjects image as the second image of tbe all pairs except the first pair.\n",
    "\t\t\telse:\n",
    "\t\t\t\ttemp_list = np.append(cat_list[:i], cat_list[i+1:])\n",
    "\t\t\t\ttemp_list = np.random.choice(temp_list)\n",
    "\t\t\t\ttemp[1][k] = X_data[np.random.choice(temp_list)]\n",
    "\n",
    "\t\tresult = model.predict(temp)\n",
    "# \t\tresult = np.argmax(result, axis=1).tolist()\n",
    "# \t\ty_true = [1 for ]\n",
    "# \t\taccuracy = accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
    "\t\t\n",
    "# \t\treturn N\n",
    "\t\tresult = result.flatten().tolist()\n",
    "\t\tresult_index = result.index(min(result))\n",
    "\t\tif result_index == 0:\n",
    "\t\t\tn_correct = n_correct + 1\n",
    "\tprint(n_correct, \" correctly classified among \", n_val)\n",
    "\taccuracy = (n_correct*100)/n_val\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_with_batch(model, X_train,cat_train,train_size,epochs,enc,n_way = 20,n_val = 64,batch_size = 64):\n",
    "\tloss_list=[]\n",
    "\taccuracy_list=[]\n",
    "\tfor epoch in range(1,epochs):\n",
    "\t\tbatch_x, batch_y = get_batch(X_train, cat_train, train_size, batch_size)\n",
    "# \t\tbatch_y = enc.fit_transform(batch_y.reshape(-1,1))\n",
    "\t\tloss = model.train_on_batch(batch_x, batch_y)\n",
    "\t\tloss_list.append((epoch,loss))\n",
    "\t\tprint('Epoch:', epoch, ', Loss:',loss)\n",
    "\t\tif epoch%100 == 0:\n",
    "\t\t\tprint(\"=============================================\")\n",
    "\t\t\taccuracy = one_shot_learning(model, n_way, n_val)\n",
    "\t\t\taccuracy_list.append((epoch, accuracy))\n",
    "\t\t\tprint('Accuracy as of', epoch, 'epochs:', accuracy)\n",
    "\t\t\tprint(\"=============================================\")\n",
    "\t\t\tif(accuracy>99):\n",
    "\t\t\t\tprint(\"Achieved more than 90% Accuracy\")\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: [5.659049987792969, 0.51171875]\n",
      "Epoch: 2 , Loss: [5.626762390136719, 0.5078125]\n",
      "Epoch: 3 , Loss: [5.58784818649292, 0.5078125]\n",
      "Epoch: 4 , Loss: [5.553286552429199, 0.5234375]\n",
      "Epoch: 5 , Loss: [5.525815010070801, 0.44140625]\n",
      "Epoch: 6 , Loss: [5.475176811218262, 0.5390625]\n",
      "Epoch: 7 , Loss: [5.449907302856445, 0.49609375]\n",
      "Epoch: 8 , Loss: [5.409162521362305, 0.55078125]\n",
      "Epoch: 9 , Loss: [5.383829116821289, 0.5]\n",
      "Epoch: 10 , Loss: [5.344635963439941, 0.51953125]\n",
      "Epoch: 11 , Loss: [5.312382698059082, 0.53515625]\n",
      "Epoch: 12 , Loss: [5.280072212219238, 0.55078125]\n",
      "Epoch: 13 , Loss: [5.24574613571167, 0.53515625]\n",
      "Epoch: 14 , Loss: [5.209768295288086, 0.55078125]\n",
      "Epoch: 15 , Loss: [5.179605484008789, 0.546875]\n",
      "Epoch: 16 , Loss: [5.149595260620117, 0.4921875]\n",
      "Epoch: 17 , Loss: [5.120455741882324, 0.4765625]\n",
      "Epoch: 18 , Loss: [5.076554298400879, 0.5859375]\n",
      "Epoch: 19 , Loss: [5.057098388671875, 0.5078125]\n",
      "Epoch: 20 , Loss: [5.0156145095825195, 0.52734375]\n",
      "Epoch: 21 , Loss: [4.979024887084961, 0.6015625]\n",
      "Epoch: 22 , Loss: [4.940582752227783, 0.609375]\n",
      "Epoch: 23 , Loss: [4.921396732330322, 0.55078125]\n",
      "Epoch: 24 , Loss: [4.887701988220215, 0.56640625]\n",
      "Epoch: 25 , Loss: [4.861945152282715, 0.53125]\n",
      "Epoch: 26 , Loss: [4.833943843841553, 0.52734375]\n",
      "Epoch: 27 , Loss: [4.805339813232422, 0.4765625]\n",
      "Epoch: 28 , Loss: [4.768019199371338, 0.55859375]\n",
      "Epoch: 29 , Loss: [4.741352081298828, 0.50390625]\n",
      "Epoch: 30 , Loss: [4.702829360961914, 0.60546875]\n",
      "Epoch: 31 , Loss: [4.674631118774414, 0.5625]\n",
      "Epoch: 32 , Loss: [4.647066116333008, 0.57421875]\n",
      "Epoch: 33 , Loss: [4.617647171020508, 0.5390625]\n",
      "Epoch: 34 , Loss: [4.58491849899292, 0.55078125]\n",
      "Epoch: 35 , Loss: [4.55933141708374, 0.546875]\n",
      "Epoch: 36 , Loss: [4.533953666687012, 0.54296875]\n",
      "Epoch: 37 , Loss: [4.489354133605957, 0.60546875]\n",
      "Epoch: 38 , Loss: [4.464724540710449, 0.5859375]\n",
      "Epoch: 39 , Loss: [4.43766450881958, 0.578125]\n",
      "Epoch: 40 , Loss: [4.4107279777526855, 0.58203125]\n",
      "Epoch: 41 , Loss: [4.385105133056641, 0.5859375]\n",
      "Epoch: 42 , Loss: [4.355149269104004, 0.6015625]\n",
      "Epoch: 43 , Loss: [4.32635498046875, 0.59375]\n",
      "Epoch: 44 , Loss: [4.29625940322876, 0.63671875]\n",
      "Epoch: 45 , Loss: [4.269275188446045, 0.62890625]\n",
      "Epoch: 46 , Loss: [4.243121147155762, 0.6171875]\n",
      "Epoch: 47 , Loss: [4.215094089508057, 0.640625]\n",
      "Epoch: 48 , Loss: [4.190891742706299, 0.61328125]\n",
      "Epoch: 49 , Loss: [4.160154342651367, 0.64453125]\n",
      "Epoch: 50 , Loss: [4.133543491363525, 0.6484375]\n",
      "Epoch: 51 , Loss: [4.110862731933594, 0.625]\n",
      "Epoch: 52 , Loss: [4.088927268981934, 0.5859375]\n",
      "Epoch: 53 , Loss: [4.06055212020874, 0.59765625]\n",
      "Epoch: 54 , Loss: [4.0264058113098145, 0.66796875]\n",
      "Epoch: 55 , Loss: [4.002871036529541, 0.70703125]\n",
      "Epoch: 56 , Loss: [3.977696180343628, 0.63671875]\n",
      "Epoch: 57 , Loss: [3.956200361251831, 0.6171875]\n",
      "Epoch: 58 , Loss: [3.930243968963623, 0.65234375]\n",
      "Epoch: 59 , Loss: [3.898858070373535, 0.66796875]\n",
      "Epoch: 60 , Loss: [3.879889488220215, 0.64453125]\n",
      "Epoch: 61 , Loss: [3.8521430492401123, 0.6796875]\n",
      "Epoch: 62 , Loss: [3.8300137519836426, 0.6875]\n",
      "Epoch: 63 , Loss: [3.8091464042663574, 0.66796875]\n",
      "Epoch: 64 , Loss: [3.7843918800354004, 0.703125]\n",
      "Epoch: 65 , Loss: [3.754897117614746, 0.67578125]\n",
      "Epoch: 66 , Loss: [3.7288055419921875, 0.66796875]\n",
      "Epoch: 67 , Loss: [3.720299482345581, 0.59375]\n",
      "Epoch: 68 , Loss: [3.696652412414551, 0.61328125]\n",
      "Epoch: 69 , Loss: [3.6596360206604004, 0.69921875]\n",
      "Epoch: 70 , Loss: [3.634620428085327, 0.6796875]\n",
      "Epoch: 71 , Loss: [3.6197612285614014, 0.65625]\n",
      "Epoch: 72 , Loss: [3.593395709991455, 0.66015625]\n",
      "Epoch: 73 , Loss: [3.5703048706054688, 0.6796875]\n",
      "Epoch: 74 , Loss: [3.5508675575256348, 0.67578125]\n",
      "Epoch: 75 , Loss: [3.523955821990967, 0.73046875]\n",
      "Epoch: 76 , Loss: [3.5069780349731445, 0.66796875]\n",
      "Epoch: 77 , Loss: [3.4830031394958496, 0.66796875]\n",
      "Epoch: 78 , Loss: [3.461268424987793, 0.65625]\n",
      "Epoch: 79 , Loss: [3.4453015327453613, 0.65234375]\n",
      "Epoch: 80 , Loss: [3.4107840061187744, 0.75390625]\n",
      "Epoch: 81 , Loss: [3.3914082050323486, 0.72265625]\n",
      "Epoch: 82 , Loss: [3.367520332336426, 0.73828125]\n",
      "Epoch: 83 , Loss: [3.35072660446167, 0.75]\n",
      "Epoch: 84 , Loss: [3.3275203704833984, 0.7109375]\n",
      "Epoch: 85 , Loss: [3.3091118335723877, 0.73828125]\n",
      "Epoch: 86 , Loss: [3.284418821334839, 0.74609375]\n",
      "Epoch: 87 , Loss: [3.262251853942871, 0.7578125]\n",
      "Epoch: 88 , Loss: [3.244943141937256, 0.75]\n",
      "Epoch: 89 , Loss: [3.220594882965088, 0.76953125]\n",
      "Epoch: 90 , Loss: [3.202725887298584, 0.75390625]\n",
      "Epoch: 91 , Loss: [3.184487819671631, 0.7265625]\n",
      "Epoch: 92 , Loss: [3.1544089317321777, 0.8203125]\n",
      "Epoch: 93 , Loss: [3.1382055282592773, 0.7734375]\n",
      "Epoch: 94 , Loss: [3.1267151832580566, 0.765625]\n",
      "Epoch: 95 , Loss: [3.0985546112060547, 0.7890625]\n",
      "Epoch: 96 , Loss: [3.0954601764678955, 0.6953125]\n",
      "Epoch: 97 , Loss: [3.06354022026062, 0.74609375]\n",
      "Epoch: 98 , Loss: [3.0403552055358887, 0.78125]\n",
      "Epoch: 99 , Loss: [3.0192482471466064, 0.8046875]\n",
      "Epoch: 100 , Loss: [3.0038647651672363, 0.75390625]\n",
      "=============================================\n",
      "13  correctly classified among  64\n",
      "Accuracy as of 100 epochs: 20.3125\n",
      "=============================================\n",
      "Epoch: 101 , Loss: [2.9853219985961914, 0.76171875]\n",
      "Epoch: 102 , Loss: [2.9653689861297607, 0.765625]\n",
      "Epoch: 103 , Loss: [2.9535491466522217, 0.734375]\n",
      "Epoch: 104 , Loss: [2.9236271381378174, 0.8046875]\n",
      "Epoch: 105 , Loss: [2.919215202331543, 0.73828125]\n",
      "Epoch: 106 , Loss: [2.8965377807617188, 0.76953125]\n",
      "Epoch: 107 , Loss: [2.869670867919922, 0.796875]\n",
      "Epoch: 108 , Loss: [2.8531908988952637, 0.76953125]\n",
      "Epoch: 109 , Loss: [2.8401951789855957, 0.75]\n",
      "Epoch: 110 , Loss: [2.8190102577209473, 0.8359375]\n",
      "Epoch: 111 , Loss: [2.799691677093506, 0.7734375]\n",
      "Epoch: 112 , Loss: [2.7858152389526367, 0.7734375]\n",
      "Epoch: 113 , Loss: [2.764188289642334, 0.828125]\n",
      "Epoch: 114 , Loss: [2.748537540435791, 0.8046875]\n",
      "Epoch: 115 , Loss: [2.736361503601074, 0.7421875]\n",
      "Epoch: 116 , Loss: [2.707693099975586, 0.79296875]\n",
      "Epoch: 117 , Loss: [2.705115795135498, 0.7421875]\n",
      "Epoch: 118 , Loss: [2.6832661628723145, 0.81640625]\n",
      "Epoch: 119 , Loss: [2.66560435295105, 0.7578125]\n",
      "Epoch: 120 , Loss: [2.6466145515441895, 0.7890625]\n",
      "Epoch: 121 , Loss: [2.61940336227417, 0.859375]\n",
      "Epoch: 122 , Loss: [2.616023063659668, 0.78515625]\n",
      "Epoch: 123 , Loss: [2.6026978492736816, 0.81640625]\n",
      "Epoch: 124 , Loss: [2.575896739959717, 0.8359375]\n",
      "Epoch: 125 , Loss: [2.5785622596740723, 0.79296875]\n",
      "Epoch: 126 , Loss: [2.542748212814331, 0.84375]\n",
      "Epoch: 127 , Loss: [2.523383617401123, 0.8125]\n",
      "Epoch: 128 , Loss: [2.513796806335449, 0.82421875]\n",
      "Epoch: 129 , Loss: [2.499941110610962, 0.828125]\n",
      "Epoch: 130 , Loss: [2.489048480987549, 0.796875]\n",
      "Epoch: 131 , Loss: [2.467374086380005, 0.84375]\n",
      "Epoch: 132 , Loss: [2.464144706726074, 0.828125]\n",
      "Epoch: 133 , Loss: [2.4453179836273193, 0.79296875]\n",
      "Epoch: 134 , Loss: [2.4222419261932373, 0.83984375]\n",
      "Epoch: 135 , Loss: [2.410320520401001, 0.8125]\n",
      "Epoch: 136 , Loss: [2.3938815593719482, 0.8125]\n",
      "Epoch: 137 , Loss: [2.371506690979004, 0.84375]\n",
      "Epoch: 138 , Loss: [2.3541600704193115, 0.8125]\n",
      "Epoch: 139 , Loss: [2.3460285663604736, 0.86328125]\n",
      "Epoch: 140 , Loss: [2.337137222290039, 0.8125]\n",
      "Epoch: 141 , Loss: [2.3144469261169434, 0.8359375]\n",
      "Epoch: 142 , Loss: [2.295389413833618, 0.8671875]\n",
      "Epoch: 143 , Loss: [2.288343906402588, 0.8515625]\n",
      "Epoch: 144 , Loss: [2.2792320251464844, 0.8203125]\n",
      "Epoch: 145 , Loss: [2.2685775756835938, 0.83203125]\n",
      "Epoch: 146 , Loss: [2.250546455383301, 0.82421875]\n",
      "Epoch: 147 , Loss: [2.2256100177764893, 0.8671875]\n",
      "Epoch: 148 , Loss: [2.2227866649627686, 0.8359375]\n",
      "Epoch: 149 , Loss: [2.2116189002990723, 0.8359375]\n",
      "Epoch: 150 , Loss: [2.186933994293213, 0.84765625]\n",
      "Epoch: 151 , Loss: [2.177177667617798, 0.84765625]\n",
      "Epoch: 152 , Loss: [2.1629230976104736, 0.83984375]\n",
      "Epoch: 153 , Loss: [2.149630069732666, 0.828125]\n",
      "Epoch: 154 , Loss: [2.1369807720184326, 0.84765625]\n",
      "Epoch: 155 , Loss: [2.1233909130096436, 0.84765625]\n",
      "Epoch: 156 , Loss: [2.1121392250061035, 0.859375]\n",
      "Epoch: 157 , Loss: [2.096967935562134, 0.83984375]\n",
      "Epoch: 158 , Loss: [2.0871505737304688, 0.8203125]\n",
      "Epoch: 159 , Loss: [2.077597141265869, 0.83984375]\n",
      "Epoch: 160 , Loss: [2.05704665184021, 0.8515625]\n",
      "Epoch: 161 , Loss: [2.0385892391204834, 0.8671875]\n",
      "Epoch: 162 , Loss: [2.0218682289123535, 0.8828125]\n",
      "Epoch: 163 , Loss: [2.0255746841430664, 0.8515625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164 , Loss: [2.0045113563537598, 0.84375]\n",
      "Epoch: 165 , Loss: [2.0037717819213867, 0.859375]\n",
      "Epoch: 166 , Loss: [1.9798531532287598, 0.83203125]\n",
      "Epoch: 167 , Loss: [1.9822005033493042, 0.8203125]\n",
      "Epoch: 168 , Loss: [1.9485087394714355, 0.8984375]\n",
      "Epoch: 169 , Loss: [1.9502220153808594, 0.84375]\n",
      "Epoch: 170 , Loss: [1.934490442276001, 0.8515625]\n",
      "Epoch: 171 , Loss: [1.921012282371521, 0.87109375]\n",
      "Epoch: 172 , Loss: [1.9158368110656738, 0.84765625]\n",
      "Epoch: 173 , Loss: [1.896485447883606, 0.84765625]\n",
      "Epoch: 174 , Loss: [1.881333589553833, 0.87890625]\n",
      "Epoch: 175 , Loss: [1.8939390182495117, 0.80078125]\n",
      "Epoch: 176 , Loss: [1.8810064792633057, 0.8125]\n",
      "Epoch: 177 , Loss: [1.8594677448272705, 0.84765625]\n",
      "Epoch: 178 , Loss: [1.8593425750732422, 0.8125]\n",
      "Epoch: 179 , Loss: [1.8397760391235352, 0.8515625]\n",
      "Epoch: 180 , Loss: [1.8278768062591553, 0.828125]\n",
      "Epoch: 181 , Loss: [1.8194842338562012, 0.85546875]\n",
      "Epoch: 182 , Loss: [1.8039835691452026, 0.82421875]\n",
      "Epoch: 183 , Loss: [1.784883737564087, 0.85546875]\n",
      "Epoch: 184 , Loss: [1.7794464826583862, 0.84375]\n",
      "Epoch: 185 , Loss: [1.7689595222473145, 0.82421875]\n",
      "Epoch: 186 , Loss: [1.7600219249725342, 0.8828125]\n",
      "Epoch: 187 , Loss: [1.7299562692642212, 0.859375]\n",
      "Epoch: 188 , Loss: [1.747297763824463, 0.80859375]\n",
      "Epoch: 189 , Loss: [1.7310402393341064, 0.84375]\n",
      "Epoch: 190 , Loss: [1.7256351709365845, 0.8359375]\n",
      "Epoch: 191 , Loss: [1.7038724422454834, 0.859375]\n",
      "Epoch: 192 , Loss: [1.6922602653503418, 0.8203125]\n",
      "Epoch: 193 , Loss: [1.7094943523406982, 0.84765625]\n",
      "Epoch: 194 , Loss: [1.6740148067474365, 0.85546875]\n",
      "Epoch: 195 , Loss: [1.6714130640029907, 0.84375]\n",
      "Epoch: 196 , Loss: [1.6665256023406982, 0.83203125]\n",
      "Epoch: 197 , Loss: [1.6539063453674316, 0.8046875]\n",
      "Epoch: 198 , Loss: [1.6400833129882812, 0.81640625]\n",
      "Epoch: 199 , Loss: [1.6249237060546875, 0.84375]\n",
      "Epoch: 200 , Loss: [1.6238057613372803, 0.87890625]\n",
      "=============================================\n",
      "21  correctly classified among  64\n",
      "Accuracy as of 200 epochs: 32.8125\n",
      "=============================================\n",
      "Epoch: 201 , Loss: [1.6202735900878906, 0.8515625]\n",
      "Epoch: 202 , Loss: [1.6061344146728516, 0.83984375]\n",
      "Epoch: 203 , Loss: [1.5843408107757568, 0.84765625]\n",
      "Epoch: 204 , Loss: [1.5889887809753418, 0.84765625]\n",
      "Epoch: 205 , Loss: [1.5735939741134644, 0.8671875]\n",
      "Epoch: 206 , Loss: [1.5695288181304932, 0.83203125]\n",
      "Epoch: 207 , Loss: [1.5476505756378174, 0.8671875]\n",
      "Epoch: 208 , Loss: [1.5441774129867554, 0.84375]\n",
      "Epoch: 209 , Loss: [1.5346163511276245, 0.8203125]\n",
      "Epoch: 210 , Loss: [1.518304705619812, 0.875]\n",
      "Epoch: 211 , Loss: [1.53105628490448, 0.828125]\n",
      "Epoch: 212 , Loss: [1.52639901638031, 0.79296875]\n",
      "Epoch: 213 , Loss: [1.502689242362976, 0.83203125]\n",
      "Epoch: 214 , Loss: [1.507305383682251, 0.8125]\n",
      "Epoch: 215 , Loss: [1.4939768314361572, 0.82421875]\n",
      "Epoch: 216 , Loss: [1.4658727645874023, 0.890625]\n",
      "Epoch: 217 , Loss: [1.4698779582977295, 0.83203125]\n",
      "Epoch: 218 , Loss: [1.4549193382263184, 0.87109375]\n",
      "Epoch: 219 , Loss: [1.4735875129699707, 0.81640625]\n",
      "Epoch: 220 , Loss: [1.4359065294265747, 0.87890625]\n",
      "Epoch: 221 , Loss: [1.4405447244644165, 0.859375]\n",
      "Epoch: 222 , Loss: [1.4367939233779907, 0.8515625]\n",
      "Epoch: 223 , Loss: [1.418526291847229, 0.8515625]\n",
      "Epoch: 224 , Loss: [1.420466661453247, 0.85546875]\n",
      "Epoch: 225 , Loss: [1.3989633321762085, 0.8359375]\n",
      "Epoch: 226 , Loss: [1.3965692520141602, 0.81640625]\n",
      "Epoch: 227 , Loss: [1.3939433097839355, 0.81640625]\n",
      "Epoch: 228 , Loss: [1.376713752746582, 0.86328125]\n",
      "Epoch: 229 , Loss: [1.3720958232879639, 0.87109375]\n",
      "Epoch: 230 , Loss: [1.359737753868103, 0.875]\n",
      "Epoch: 231 , Loss: [1.3596665859222412, 0.828125]\n",
      "Epoch: 232 , Loss: [1.3570585250854492, 0.859375]\n",
      "Epoch: 233 , Loss: [1.3331831693649292, 0.8828125]\n",
      "Epoch: 234 , Loss: [1.3275141716003418, 0.86328125]\n",
      "Epoch: 235 , Loss: [1.3453376293182373, 0.8203125]\n",
      "Epoch: 236 , Loss: [1.3458061218261719, 0.78515625]\n",
      "Epoch: 237 , Loss: [1.3160864114761353, 0.82421875]\n",
      "Epoch: 238 , Loss: [1.298244595527649, 0.8359375]\n",
      "Epoch: 239 , Loss: [1.3134794235229492, 0.83203125]\n",
      "Epoch: 240 , Loss: [1.2897005081176758, 0.8515625]\n",
      "Epoch: 241 , Loss: [1.2943506240844727, 0.828125]\n",
      "Epoch: 242 , Loss: [1.2672518491744995, 0.86328125]\n",
      "Epoch: 243 , Loss: [1.2590487003326416, 0.86328125]\n",
      "Epoch: 244 , Loss: [1.2464561462402344, 0.86328125]\n",
      "Epoch: 245 , Loss: [1.2508282661437988, 0.8671875]\n",
      "Epoch: 246 , Loss: [1.2489014863967896, 0.86328125]\n",
      "Epoch: 247 , Loss: [1.244482159614563, 0.8359375]\n",
      "Epoch: 248 , Loss: [1.2301251888275146, 0.875]\n",
      "Epoch: 249 , Loss: [1.2420921325683594, 0.83984375]\n",
      "Epoch: 250 , Loss: [1.2258332967758179, 0.83984375]\n",
      "Epoch: 251 , Loss: [1.211755394935608, 0.84375]\n",
      "Epoch: 252 , Loss: [1.2044196128845215, 0.84765625]\n",
      "Epoch: 253 , Loss: [1.2055317163467407, 0.85546875]\n",
      "Epoch: 254 , Loss: [1.2161831855773926, 0.8203125]\n",
      "Epoch: 255 , Loss: [1.2106349468231201, 0.83984375]\n",
      "Epoch: 256 , Loss: [1.1846532821655273, 0.8359375]\n",
      "Epoch: 257 , Loss: [1.1904467344284058, 0.83203125]\n",
      "Epoch: 258 , Loss: [1.209093451499939, 0.796875]\n",
      "Epoch: 259 , Loss: [1.1707879304885864, 0.84375]\n",
      "Epoch: 260 , Loss: [1.177145004272461, 0.87109375]\n",
      "Epoch: 261 , Loss: [1.1409649848937988, 0.8671875]\n",
      "Epoch: 262 , Loss: [1.1692900657653809, 0.83203125]\n",
      "Epoch: 263 , Loss: [1.1714351177215576, 0.84375]\n",
      "Epoch: 264 , Loss: [1.1254985332489014, 0.8671875]\n",
      "Epoch: 265 , Loss: [1.1292102336883545, 0.8515625]\n",
      "Epoch: 266 , Loss: [1.1493611335754395, 0.81640625]\n",
      "Epoch: 267 , Loss: [1.1423370838165283, 0.828125]\n",
      "Epoch: 268 , Loss: [1.1188724040985107, 0.83984375]\n",
      "Epoch: 269 , Loss: [1.1391949653625488, 0.796875]\n",
      "Epoch: 270 , Loss: [1.11000394821167, 0.84765625]\n",
      "Epoch: 271 , Loss: [1.0994983911514282, 0.85546875]\n",
      "Epoch: 272 , Loss: [1.1151119470596313, 0.83203125]\n",
      "Epoch: 273 , Loss: [1.0771825313568115, 0.83984375]\n",
      "Epoch: 274 , Loss: [1.1098310947418213, 0.828125]\n",
      "Epoch: 275 , Loss: [1.0816130638122559, 0.83203125]\n",
      "Epoch: 276 , Loss: [1.0887699127197266, 0.8515625]\n",
      "Epoch: 277 , Loss: [1.0727750062942505, 0.84765625]\n",
      "Epoch: 278 , Loss: [1.0768532752990723, 0.83984375]\n",
      "Epoch: 279 , Loss: [1.0741517543792725, 0.84375]\n",
      "Epoch: 280 , Loss: [1.0631260871887207, 0.859375]\n",
      "Epoch: 281 , Loss: [1.0700280666351318, 0.83984375]\n",
      "Epoch: 282 , Loss: [1.0420029163360596, 0.8359375]\n",
      "Epoch: 283 , Loss: [1.03671133518219, 0.8515625]\n",
      "Epoch: 284 , Loss: [1.0423698425292969, 0.84765625]\n",
      "Epoch: 285 , Loss: [1.0355195999145508, 0.8671875]\n",
      "Epoch: 286 , Loss: [1.041804552078247, 0.8359375]\n",
      "Epoch: 287 , Loss: [1.0507733821868896, 0.8359375]\n",
      "Epoch: 288 , Loss: [1.025999665260315, 0.85546875]\n",
      "Epoch: 289 , Loss: [1.0253931283950806, 0.81640625]\n",
      "Epoch: 290 , Loss: [1.0101162195205688, 0.82421875]\n",
      "Epoch: 291 , Loss: [1.0173768997192383, 0.83203125]\n",
      "Epoch: 292 , Loss: [1.0111103057861328, 0.83984375]\n",
      "Epoch: 293 , Loss: [1.000644326210022, 0.828125]\n",
      "Epoch: 294 , Loss: [1.0079703330993652, 0.8203125]\n",
      "Epoch: 295 , Loss: [0.9916447401046753, 0.84375]\n",
      "Epoch: 296 , Loss: [0.9606999158859253, 0.875]\n",
      "Epoch: 297 , Loss: [1.006052017211914, 0.8046875]\n",
      "Epoch: 298 , Loss: [0.9680823087692261, 0.890625]\n",
      "Epoch: 299 , Loss: [0.9691510796546936, 0.84765625]\n",
      "Epoch: 300 , Loss: [0.9802870750427246, 0.81640625]\n",
      "=============================================\n",
      "16  correctly classified among  64\n",
      "Accuracy as of 300 epochs: 25.0\n",
      "=============================================\n",
      "Epoch: 301 , Loss: [0.9771252870559692, 0.82421875]\n",
      "Epoch: 302 , Loss: [0.9572948217391968, 0.84375]\n",
      "Epoch: 303 , Loss: [0.9436250329017639, 0.859375]\n",
      "Epoch: 304 , Loss: [0.9440833330154419, 0.890625]\n",
      "Epoch: 305 , Loss: [0.9371927976608276, 0.83984375]\n",
      "Epoch: 306 , Loss: [0.9464431405067444, 0.828125]\n",
      "Epoch: 307 , Loss: [0.9134053587913513, 0.87890625]\n",
      "Epoch: 308 , Loss: [0.9561389684677124, 0.8359375]\n",
      "Epoch: 309 , Loss: [0.9235577583312988, 0.859375]\n",
      "Epoch: 310 , Loss: [0.9333491921424866, 0.84375]\n",
      "Epoch: 311 , Loss: [0.918076753616333, 0.84375]\n",
      "Epoch: 312 , Loss: [0.908606231212616, 0.859375]\n",
      "Epoch: 313 , Loss: [0.8756531476974487, 0.890625]\n",
      "Epoch: 314 , Loss: [0.9287424683570862, 0.828125]\n",
      "Epoch: 315 , Loss: [0.8748263120651245, 0.8828125]\n",
      "Epoch: 316 , Loss: [0.898402750492096, 0.83984375]\n",
      "Epoch: 317 , Loss: [0.9056041240692139, 0.83984375]\n",
      "Epoch: 318 , Loss: [0.896539568901062, 0.83203125]\n",
      "Epoch: 319 , Loss: [0.8743350505828857, 0.859375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 320 , Loss: [0.8781841397285461, 0.87109375]\n",
      "Epoch: 321 , Loss: [0.8765413761138916, 0.86328125]\n",
      "Epoch: 322 , Loss: [0.874678373336792, 0.8359375]\n",
      "Epoch: 323 , Loss: [0.8797109127044678, 0.84765625]\n",
      "Epoch: 324 , Loss: [0.8638777136802673, 0.84375]\n",
      "Epoch: 325 , Loss: [0.8667126893997192, 0.8515625]\n",
      "Epoch: 326 , Loss: [0.8985646963119507, 0.83203125]\n",
      "Epoch: 327 , Loss: [0.8648933172225952, 0.84765625]\n",
      "Epoch: 328 , Loss: [0.8689184784889221, 0.8515625]\n",
      "Epoch: 329 , Loss: [0.8332463502883911, 0.84765625]\n",
      "Epoch: 330 , Loss: [0.8480620384216309, 0.87890625]\n",
      "Epoch: 331 , Loss: [0.8417923450469971, 0.84375]\n",
      "Epoch: 332 , Loss: [0.8728058934211731, 0.83203125]\n",
      "Epoch: 333 , Loss: [0.839984655380249, 0.84765625]\n",
      "Epoch: 334 , Loss: [0.80476975440979, 0.890625]\n",
      "Epoch: 335 , Loss: [0.832095205783844, 0.84765625]\n",
      "Epoch: 336 , Loss: [0.8312044739723206, 0.8125]\n",
      "Epoch: 337 , Loss: [0.8249056339263916, 0.85546875]\n",
      "Epoch: 338 , Loss: [0.812190592288971, 0.88671875]\n",
      "Epoch: 339 , Loss: [0.8210231065750122, 0.83203125]\n",
      "Epoch: 340 , Loss: [0.8504845499992371, 0.8125]\n",
      "Epoch: 341 , Loss: [0.8277602195739746, 0.83203125]\n",
      "Epoch: 342 , Loss: [0.8119523525238037, 0.85546875]\n",
      "Epoch: 343 , Loss: [0.7937091588973999, 0.8671875]\n",
      "Epoch: 344 , Loss: [0.801412045955658, 0.84375]\n",
      "Epoch: 345 , Loss: [0.8086534738540649, 0.85546875]\n",
      "Epoch: 346 , Loss: [0.809677004814148, 0.8359375]\n",
      "Epoch: 347 , Loss: [0.7836441993713379, 0.8515625]\n",
      "Epoch: 348 , Loss: [0.7660984992980957, 0.91796875]\n",
      "Epoch: 349 , Loss: [0.791303277015686, 0.86328125]\n",
      "Epoch: 350 , Loss: [0.7998825311660767, 0.8515625]\n",
      "Epoch: 351 , Loss: [0.7973693013191223, 0.828125]\n",
      "Epoch: 352 , Loss: [0.7853108644485474, 0.86328125]\n",
      "Epoch: 353 , Loss: [0.7706459760665894, 0.83203125]\n",
      "Epoch: 354 , Loss: [0.7789326310157776, 0.8515625]\n",
      "Epoch: 355 , Loss: [0.7869715094566345, 0.8515625]\n",
      "Epoch: 356 , Loss: [0.7780647277832031, 0.83984375]\n",
      "Epoch: 357 , Loss: [0.7353042364120483, 0.88671875]\n",
      "Epoch: 358 , Loss: [0.7504626512527466, 0.828125]\n",
      "Epoch: 359 , Loss: [0.738776445388794, 0.87890625]\n",
      "Epoch: 360 , Loss: [0.7521036863327026, 0.86328125]\n",
      "Epoch: 361 , Loss: [0.7530339956283569, 0.859375]\n",
      "Epoch: 362 , Loss: [0.7709181308746338, 0.828125]\n",
      "Epoch: 363 , Loss: [0.739130437374115, 0.87109375]\n",
      "Epoch: 364 , Loss: [0.7237856984138489, 0.86328125]\n",
      "Epoch: 365 , Loss: [0.7522928714752197, 0.85546875]\n",
      "Epoch: 366 , Loss: [0.768464982509613, 0.83203125]\n",
      "Epoch: 367 , Loss: [0.7386537194252014, 0.8515625]\n",
      "Epoch: 368 , Loss: [0.7457152009010315, 0.8515625]\n",
      "Epoch: 369 , Loss: [0.7511318325996399, 0.83203125]\n",
      "Epoch: 370 , Loss: [0.7111561894416809, 0.90234375]\n",
      "Epoch: 371 , Loss: [0.7298163175582886, 0.8671875]\n",
      "Epoch: 372 , Loss: [0.7372536659240723, 0.828125]\n",
      "Epoch: 373 , Loss: [0.7451212406158447, 0.83203125]\n",
      "Epoch: 374 , Loss: [0.7338368892669678, 0.8515625]\n",
      "Epoch: 375 , Loss: [0.7094857692718506, 0.859375]\n",
      "Epoch: 376 , Loss: [0.7263219952583313, 0.83203125]\n",
      "Epoch: 377 , Loss: [0.7242207527160645, 0.859375]\n",
      "Epoch: 378 , Loss: [0.7332949638366699, 0.84375]\n",
      "Epoch: 379 , Loss: [0.6947852969169617, 0.85546875]\n",
      "Epoch: 380 , Loss: [0.7150133848190308, 0.84765625]\n",
      "Epoch: 381 , Loss: [0.7169652581214905, 0.84765625]\n",
      "Epoch: 382 , Loss: [0.6987629532814026, 0.87890625]\n",
      "Epoch: 383 , Loss: [0.715579628944397, 0.8359375]\n",
      "Epoch: 384 , Loss: [0.6937693357467651, 0.84375]\n",
      "Epoch: 385 , Loss: [0.7058846354484558, 0.875]\n",
      "Epoch: 386 , Loss: [0.6920179128646851, 0.83203125]\n",
      "Epoch: 387 , Loss: [0.6973748207092285, 0.8359375]\n",
      "Epoch: 388 , Loss: [0.7147659659385681, 0.8359375]\n",
      "Epoch: 389 , Loss: [0.6985534429550171, 0.8515625]\n",
      "Epoch: 390 , Loss: [0.6784955859184265, 0.8828125]\n",
      "Epoch: 391 , Loss: [0.7182009816169739, 0.8125]\n",
      "Epoch: 392 , Loss: [0.679634690284729, 0.8671875]\n",
      "Epoch: 393 , Loss: [0.661760151386261, 0.859375]\n",
      "Epoch: 394 , Loss: [0.6629428267478943, 0.890625]\n",
      "Epoch: 395 , Loss: [0.6508467197418213, 0.890625]\n",
      "Epoch: 396 , Loss: [0.7096754312515259, 0.82421875]\n",
      "Epoch: 397 , Loss: [0.6984320878982544, 0.828125]\n",
      "Epoch: 398 , Loss: [0.6675123572349548, 0.8671875]\n",
      "Epoch: 399 , Loss: [0.6726657748222351, 0.8359375]\n",
      "Epoch: 400 , Loss: [0.6494808793067932, 0.89453125]\n",
      "=============================================\n",
      "24  correctly classified among  64\n",
      "Accuracy as of 400 epochs: 37.5\n",
      "=============================================\n",
      "Epoch: 401 , Loss: [0.6480538249015808, 0.86328125]\n",
      "Epoch: 402 , Loss: [0.6504649519920349, 0.859375]\n",
      "Epoch: 403 , Loss: [0.6566059589385986, 0.8671875]\n",
      "Epoch: 404 , Loss: [0.659339964389801, 0.84375]\n",
      "Epoch: 405 , Loss: [0.6689400672912598, 0.8671875]\n",
      "Epoch: 406 , Loss: [0.6828608512878418, 0.85546875]\n",
      "Epoch: 407 , Loss: [0.6689807176589966, 0.84765625]\n",
      "Epoch: 408 , Loss: [0.6569164991378784, 0.8515625]\n",
      "Epoch: 409 , Loss: [0.6334378123283386, 0.86328125]\n",
      "Epoch: 410 , Loss: [0.6538838148117065, 0.859375]\n",
      "Epoch: 411 , Loss: [0.6318701505661011, 0.875]\n",
      "Epoch: 412 , Loss: [0.6508959531784058, 0.84765625]\n",
      "Epoch: 413 , Loss: [0.6430574059486389, 0.86328125]\n",
      "Epoch: 414 , Loss: [0.6812382340431213, 0.828125]\n",
      "Epoch: 415 , Loss: [0.643688976764679, 0.8671875]\n",
      "Epoch: 416 , Loss: [0.649343729019165, 0.875]\n",
      "Epoch: 417 , Loss: [0.6496469378471375, 0.828125]\n",
      "Epoch: 418 , Loss: [0.6099870800971985, 0.8828125]\n",
      "Epoch: 419 , Loss: [0.6328396201133728, 0.85546875]\n",
      "Epoch: 420 , Loss: [0.6467433571815491, 0.859375]\n",
      "Epoch: 421 , Loss: [0.645362377166748, 0.8671875]\n",
      "Epoch: 422 , Loss: [0.6261158585548401, 0.85546875]\n",
      "Epoch: 423 , Loss: [0.6124792098999023, 0.8828125]\n",
      "Epoch: 424 , Loss: [0.6367175579071045, 0.859375]\n",
      "Epoch: 425 , Loss: [0.6254059076309204, 0.88671875]\n",
      "Epoch: 426 , Loss: [0.652862548828125, 0.8046875]\n",
      "Epoch: 427 , Loss: [0.6242549419403076, 0.85546875]\n",
      "Epoch: 428 , Loss: [0.6404131054878235, 0.81640625]\n",
      "Epoch: 429 , Loss: [0.5992304682731628, 0.87109375]\n",
      "Epoch: 430 , Loss: [0.6196852922439575, 0.859375]\n",
      "Epoch: 431 , Loss: [0.6230331063270569, 0.828125]\n",
      "Epoch: 432 , Loss: [0.6192457675933838, 0.85546875]\n",
      "Epoch: 433 , Loss: [0.6135427951812744, 0.8359375]\n",
      "Epoch: 434 , Loss: [0.5720517039299011, 0.88671875]\n",
      "Epoch: 435 , Loss: [0.5884901881217957, 0.86328125]\n",
      "Epoch: 436 , Loss: [0.5926833748817444, 0.8828125]\n",
      "Epoch: 437 , Loss: [0.6005927324295044, 0.875]\n",
      "Epoch: 438 , Loss: [0.6342881917953491, 0.8125]\n",
      "Epoch: 439 , Loss: [0.5782105326652527, 0.87109375]\n",
      "Epoch: 440 , Loss: [0.6277382969856262, 0.828125]\n",
      "Epoch: 441 , Loss: [0.5733355283737183, 0.890625]\n",
      "Epoch: 442 , Loss: [0.5939397811889648, 0.84765625]\n",
      "Epoch: 443 , Loss: [0.5810555815696716, 0.89453125]\n",
      "Epoch: 444 , Loss: [0.6243051886558533, 0.8515625]\n",
      "Epoch: 445 , Loss: [0.5835786461830139, 0.8515625]\n",
      "Epoch: 446 , Loss: [0.6135229468345642, 0.859375]\n",
      "Epoch: 447 , Loss: [0.5889145731925964, 0.875]\n",
      "Epoch: 448 , Loss: [0.6003261804580688, 0.8515625]\n",
      "Epoch: 449 , Loss: [0.5743507742881775, 0.8671875]\n",
      "Epoch: 450 , Loss: [0.5864455699920654, 0.85546875]\n",
      "Epoch: 451 , Loss: [0.5829912424087524, 0.86328125]\n",
      "Epoch: 452 , Loss: [0.5691412687301636, 0.86328125]\n",
      "Epoch: 453 , Loss: [0.5626217722892761, 0.8828125]\n",
      "Epoch: 454 , Loss: [0.5577230453491211, 0.89453125]\n",
      "Epoch: 455 , Loss: [0.5930916666984558, 0.8671875]\n",
      "Epoch: 456 , Loss: [0.5609208345413208, 0.83984375]\n",
      "Epoch: 457 , Loss: [0.5785275101661682, 0.84765625]\n",
      "Epoch: 458 , Loss: [0.611538290977478, 0.82421875]\n",
      "Epoch: 459 , Loss: [0.5709830522537231, 0.875]\n",
      "Epoch: 460 , Loss: [0.5515275001525879, 0.89453125]\n",
      "Epoch: 461 , Loss: [0.5971895456314087, 0.84375]\n",
      "Epoch: 462 , Loss: [0.5902222990989685, 0.84765625]\n",
      "Epoch: 463 , Loss: [0.5506694316864014, 0.87890625]\n",
      "Epoch: 464 , Loss: [0.5708696246147156, 0.84375]\n",
      "Epoch: 465 , Loss: [0.5664480328559875, 0.875]\n",
      "Epoch: 466 , Loss: [0.5995166301727295, 0.8359375]\n",
      "Epoch: 467 , Loss: [0.530462920665741, 0.8984375]\n",
      "Epoch: 468 , Loss: [0.5560254454612732, 0.859375]\n",
      "Epoch: 469 , Loss: [0.5856561064720154, 0.8359375]\n",
      "Epoch: 470 , Loss: [0.5721480250358582, 0.859375]\n",
      "Epoch: 471 , Loss: [0.578834056854248, 0.84765625]\n",
      "Epoch: 472 , Loss: [0.5846041440963745, 0.83203125]\n",
      "Epoch: 473 , Loss: [0.5502047538757324, 0.85546875]\n",
      "Epoch: 474 , Loss: [0.5617790222167969, 0.859375]\n",
      "Epoch: 475 , Loss: [0.5692341327667236, 0.875]\n",
      "Epoch: 476 , Loss: [0.560143768787384, 0.86328125]\n",
      "Epoch: 477 , Loss: [0.5626943707466125, 0.8515625]\n",
      "Epoch: 478 , Loss: [0.5574678778648376, 0.8671875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 479 , Loss: [0.5803453326225281, 0.84375]\n",
      "Epoch: 480 , Loss: [0.5577712655067444, 0.86328125]\n",
      "Epoch: 481 , Loss: [0.5950220823287964, 0.828125]\n",
      "Epoch: 482 , Loss: [0.5215159058570862, 0.875]\n",
      "Epoch: 483 , Loss: [0.5410430431365967, 0.89453125]\n",
      "Epoch: 484 , Loss: [0.5293521285057068, 0.87109375]\n",
      "Epoch: 485 , Loss: [0.5294612646102905, 0.89453125]\n",
      "Epoch: 486 , Loss: [0.55210942029953, 0.859375]\n",
      "Epoch: 487 , Loss: [0.5341430306434631, 0.890625]\n",
      "Epoch: 488 , Loss: [0.5457583665847778, 0.86328125]\n",
      "Epoch: 489 , Loss: [0.49229663610458374, 0.921875]\n",
      "Epoch: 490 , Loss: [0.5491140484809875, 0.87109375]\n",
      "Epoch: 491 , Loss: [0.5402048230171204, 0.86328125]\n",
      "Epoch: 492 , Loss: [0.5446892976760864, 0.859375]\n",
      "Epoch: 493 , Loss: [0.5500344038009644, 0.84375]\n",
      "Epoch: 494 , Loss: [0.5477871894836426, 0.85546875]\n",
      "Epoch: 495 , Loss: [0.5559589862823486, 0.84375]\n",
      "Epoch: 496 , Loss: [0.5230649709701538, 0.8828125]\n",
      "Epoch: 497 , Loss: [0.5479225516319275, 0.828125]\n",
      "Epoch: 498 , Loss: [0.5471794605255127, 0.859375]\n",
      "Epoch: 499 , Loss: [0.5282125473022461, 0.87890625]\n",
      "Epoch: 500 , Loss: [0.5343373417854309, 0.87890625]\n",
      "=============================================\n",
      "18  correctly classified among  64\n",
      "Accuracy as of 500 epochs: 28.125\n",
      "=============================================\n",
      "Epoch: 501 , Loss: [0.5164459943771362, 0.859375]\n",
      "Epoch: 502 , Loss: [0.5571409463882446, 0.8359375]\n",
      "Epoch: 503 , Loss: [0.5367018580436707, 0.8828125]\n",
      "Epoch: 504 , Loss: [0.4993721842765808, 0.8828125]\n",
      "Epoch: 505 , Loss: [0.5330081582069397, 0.87109375]\n",
      "Epoch: 506 , Loss: [0.5340960621833801, 0.875]\n",
      "Epoch: 507 , Loss: [0.5372791290283203, 0.8515625]\n",
      "Epoch: 508 , Loss: [0.547473132610321, 0.859375]\n",
      "Epoch: 509 , Loss: [0.5353068113327026, 0.8671875]\n",
      "Epoch: 510 , Loss: [0.5495654344558716, 0.84375]\n",
      "Epoch: 511 , Loss: [0.5047804117202759, 0.87890625]\n",
      "Epoch: 512 , Loss: [0.5483440160751343, 0.8671875]\n",
      "Epoch: 513 , Loss: [0.512904703617096, 0.859375]\n",
      "Epoch: 514 , Loss: [0.5101404786109924, 0.8671875]\n",
      "Epoch: 515 , Loss: [0.5195417404174805, 0.86328125]\n",
      "Epoch: 516 , Loss: [0.5352464318275452, 0.86328125]\n",
      "Epoch: 517 , Loss: [0.5058866739273071, 0.8984375]\n",
      "Epoch: 518 , Loss: [0.5331650972366333, 0.875]\n",
      "Epoch: 519 , Loss: [0.5143770575523376, 0.8828125]\n",
      "Epoch: 520 , Loss: [0.49607762694358826, 0.890625]\n",
      "Epoch: 521 , Loss: [0.5108588933944702, 0.875]\n",
      "Epoch: 522 , Loss: [0.5320765376091003, 0.81640625]\n",
      "Epoch: 523 , Loss: [0.5126982927322388, 0.86328125]\n",
      "Epoch: 524 , Loss: [0.5563625693321228, 0.828125]\n",
      "Epoch: 525 , Loss: [0.5098839998245239, 0.87109375]\n",
      "Epoch: 526 , Loss: [0.5155519247055054, 0.875]\n",
      "Epoch: 527 , Loss: [0.4946460723876953, 0.90234375]\n",
      "Epoch: 528 , Loss: [0.49313801527023315, 0.88671875]\n",
      "Epoch: 529 , Loss: [0.49615830183029175, 0.8984375]\n",
      "Epoch: 530 , Loss: [0.5259779095649719, 0.84375]\n",
      "Epoch: 531 , Loss: [0.5366175174713135, 0.83203125]\n",
      "Epoch: 532 , Loss: [0.4936829209327698, 0.90234375]\n",
      "Epoch: 533 , Loss: [0.4884236454963684, 0.8984375]\n",
      "Epoch: 534 , Loss: [0.5114794969558716, 0.84765625]\n",
      "Epoch: 535 , Loss: [0.49845609068870544, 0.86328125]\n",
      "Epoch: 536 , Loss: [0.548244833946228, 0.83203125]\n",
      "Epoch: 537 , Loss: [0.4829120337963104, 0.8828125]\n",
      "Epoch: 538 , Loss: [0.4824545085430145, 0.90625]\n",
      "Epoch: 539 , Loss: [0.4891868531703949, 0.88671875]\n",
      "Epoch: 540 , Loss: [0.49045267701148987, 0.875]\n",
      "Epoch: 541 , Loss: [0.5187362432479858, 0.86328125]\n",
      "Epoch: 542 , Loss: [0.5027099251747131, 0.89453125]\n",
      "Epoch: 543 , Loss: [0.4876924753189087, 0.890625]\n",
      "Epoch: 544 , Loss: [0.5043758153915405, 0.8828125]\n",
      "Epoch: 545 , Loss: [0.5014682412147522, 0.875]\n",
      "Epoch: 546 , Loss: [0.4689962565898895, 0.8828125]\n",
      "Epoch: 547 , Loss: [0.48297134041786194, 0.8984375]\n",
      "Epoch: 548 , Loss: [0.5146080851554871, 0.875]\n",
      "Epoch: 549 , Loss: [0.4848617911338806, 0.890625]\n",
      "Epoch: 550 , Loss: [0.5074574947357178, 0.8515625]\n",
      "Epoch: 551 , Loss: [0.4969959855079651, 0.890625]\n",
      "Epoch: 552 , Loss: [0.4833894968032837, 0.87109375]\n",
      "Epoch: 553 , Loss: [0.49353867769241333, 0.8828125]\n",
      "Epoch: 554 , Loss: [0.4494912326335907, 0.90625]\n",
      "Epoch: 555 , Loss: [0.46783506870269775, 0.88671875]\n",
      "Epoch: 556 , Loss: [0.508568525314331, 0.83984375]\n",
      "Epoch: 557 , Loss: [0.4782176911830902, 0.88671875]\n",
      "Epoch: 558 , Loss: [0.5020676255226135, 0.85546875]\n",
      "Epoch: 559 , Loss: [0.5196720957756042, 0.859375]\n",
      "Epoch: 560 , Loss: [0.4809007942676544, 0.8984375]\n",
      "Epoch: 561 , Loss: [0.5286453366279602, 0.84375]\n",
      "Epoch: 562 , Loss: [0.5016939043998718, 0.87890625]\n",
      "Epoch: 563 , Loss: [0.4724576473236084, 0.859375]\n",
      "Epoch: 564 , Loss: [0.45775705575942993, 0.90625]\n",
      "Epoch: 565 , Loss: [0.4798870086669922, 0.875]\n",
      "Epoch: 566 , Loss: [0.4861237704753876, 0.859375]\n",
      "Epoch: 567 , Loss: [0.46660080552101135, 0.89453125]\n",
      "Epoch: 568 , Loss: [0.49618080258369446, 0.8671875]\n",
      "Epoch: 569 , Loss: [0.4954114556312561, 0.86328125]\n",
      "Epoch: 570 , Loss: [0.4829100966453552, 0.875]\n",
      "Epoch: 571 , Loss: [0.470586359500885, 0.90234375]\n",
      "Epoch: 572 , Loss: [0.45613616704940796, 0.8984375]\n",
      "Epoch: 573 , Loss: [0.4881678819656372, 0.83984375]\n",
      "Epoch: 574 , Loss: [0.47396931052207947, 0.8828125]\n",
      "Epoch: 575 , Loss: [0.4806070625782013, 0.86328125]\n",
      "Epoch: 576 , Loss: [0.4759722650051117, 0.88671875]\n",
      "Epoch: 577 , Loss: [0.4777581989765167, 0.87109375]\n",
      "Epoch: 578 , Loss: [0.484127402305603, 0.84765625]\n",
      "Epoch: 579 , Loss: [0.48798370361328125, 0.88671875]\n",
      "Epoch: 580 , Loss: [0.45405682921409607, 0.90625]\n",
      "Epoch: 581 , Loss: [0.4721021056175232, 0.88671875]\n",
      "Epoch: 582 , Loss: [0.4876258075237274, 0.859375]\n",
      "Epoch: 583 , Loss: [0.4711253345012665, 0.859375]\n",
      "Epoch: 584 , Loss: [0.4597204327583313, 0.875]\n",
      "Epoch: 585 , Loss: [0.46412238478660583, 0.8828125]\n",
      "Epoch: 586 , Loss: [0.4650464951992035, 0.875]\n",
      "Epoch: 587 , Loss: [0.4499487578868866, 0.90625]\n",
      "Epoch: 588 , Loss: [0.4739932715892792, 0.8828125]\n",
      "Epoch: 589 , Loss: [0.47437992691993713, 0.8984375]\n",
      "Epoch: 590 , Loss: [0.4490637183189392, 0.89453125]\n",
      "Epoch: 591 , Loss: [0.4506167471408844, 0.8984375]\n",
      "Epoch: 592 , Loss: [0.46405187249183655, 0.8671875]\n",
      "Epoch: 593 , Loss: [0.4786262512207031, 0.8828125]\n",
      "Epoch: 594 , Loss: [0.44379252195358276, 0.8984375]\n",
      "Epoch: 595 , Loss: [0.44120803475379944, 0.90625]\n",
      "Epoch: 596 , Loss: [0.4593965411186218, 0.88671875]\n",
      "Epoch: 597 , Loss: [0.4888731837272644, 0.8515625]\n",
      "Epoch: 598 , Loss: [0.45812317728996277, 0.87890625]\n",
      "Epoch: 599 , Loss: [0.47181248664855957, 0.859375]\n",
      "Epoch: 600 , Loss: [0.47940629720687866, 0.8671875]\n",
      "=============================================\n",
      "23  correctly classified among  64\n",
      "Accuracy as of 600 epochs: 35.9375\n",
      "=============================================\n",
      "Epoch: 601 , Loss: [0.4431973099708557, 0.8828125]\n",
      "Epoch: 602 , Loss: [0.48378005623817444, 0.84375]\n",
      "Epoch: 603 , Loss: [0.4718526005744934, 0.85546875]\n",
      "Epoch: 604 , Loss: [0.45055684447288513, 0.890625]\n",
      "Epoch: 605 , Loss: [0.43309253454208374, 0.890625]\n",
      "Epoch: 606 , Loss: [0.4718366265296936, 0.88671875]\n",
      "Epoch: 607 , Loss: [0.4375985562801361, 0.8984375]\n",
      "Epoch: 608 , Loss: [0.446591854095459, 0.890625]\n",
      "Epoch: 609 , Loss: [0.4706481397151947, 0.875]\n",
      "Epoch: 610 , Loss: [0.4294763505458832, 0.90234375]\n",
      "Epoch: 611 , Loss: [0.4702201783657074, 0.91015625]\n",
      "Epoch: 612 , Loss: [0.45809832215309143, 0.8671875]\n",
      "Epoch: 613 , Loss: [0.4747801721096039, 0.87109375]\n",
      "Epoch: 614 , Loss: [0.431199312210083, 0.9296875]\n",
      "Epoch: 615 , Loss: [0.46623942255973816, 0.87890625]\n",
      "Epoch: 616 , Loss: [0.45738083124160767, 0.859375]\n",
      "Epoch: 617 , Loss: [0.5071303248405457, 0.83203125]\n",
      "Epoch: 618 , Loss: [0.458080917596817, 0.87109375]\n",
      "Epoch: 619 , Loss: [0.4851226806640625, 0.8515625]\n",
      "Epoch: 620 , Loss: [0.4472748637199402, 0.890625]\n",
      "Epoch: 621 , Loss: [0.46561887860298157, 0.86328125]\n",
      "Epoch: 622 , Loss: [0.44106048345565796, 0.890625]\n",
      "Epoch: 623 , Loss: [0.43760812282562256, 0.90625]\n",
      "Epoch: 624 , Loss: [0.42205387353897095, 0.90625]\n",
      "Epoch: 625 , Loss: [0.437107652425766, 0.90625]\n",
      "Epoch: 626 , Loss: [0.44842272996902466, 0.87890625]\n",
      "Epoch: 627 , Loss: [0.44044312834739685, 0.91015625]\n",
      "Epoch: 628 , Loss: [0.45221877098083496, 0.89453125]\n",
      "Epoch: 629 , Loss: [0.44963791966438293, 0.87890625]\n",
      "Epoch: 630 , Loss: [0.4656369686126709, 0.8671875]\n",
      "Epoch: 631 , Loss: [0.4632590413093567, 0.87109375]\n",
      "Epoch: 632 , Loss: [0.4693325459957123, 0.859375]\n",
      "Epoch: 633 , Loss: [0.4644329845905304, 0.87109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 634 , Loss: [0.44616225361824036, 0.88671875]\n",
      "Epoch: 635 , Loss: [0.47236794233322144, 0.85546875]\n",
      "Epoch: 636 , Loss: [0.4426739513874054, 0.890625]\n",
      "Epoch: 637 , Loss: [0.4424459934234619, 0.8984375]\n",
      "Epoch: 638 , Loss: [0.45043665170669556, 0.8515625]\n",
      "Epoch: 639 , Loss: [0.44571828842163086, 0.8828125]\n",
      "Epoch: 640 , Loss: [0.44789278507232666, 0.875]\n",
      "Epoch: 641 , Loss: [0.4454038739204407, 0.87890625]\n",
      "Epoch: 642 , Loss: [0.4710657000541687, 0.8828125]\n",
      "Epoch: 643 , Loss: [0.459878146648407, 0.859375]\n",
      "Epoch: 644 , Loss: [0.4553837180137634, 0.87109375]\n",
      "Epoch: 645 , Loss: [0.45534569025039673, 0.88671875]\n",
      "Epoch: 646 , Loss: [0.46263664960861206, 0.890625]\n",
      "Epoch: 647 , Loss: [0.4388755261898041, 0.89453125]\n",
      "Epoch: 648 , Loss: [0.44479092955589294, 0.88671875]\n",
      "Epoch: 649 , Loss: [0.4349117577075958, 0.89453125]\n",
      "Epoch: 650 , Loss: [0.4226367771625519, 0.90625]\n",
      "Epoch: 651 , Loss: [0.4349710941314697, 0.890625]\n",
      "Epoch: 652 , Loss: [0.4301743805408478, 0.90234375]\n",
      "Epoch: 653 , Loss: [0.46075284481048584, 0.875]\n",
      "Epoch: 654 , Loss: [0.4236745834350586, 0.88671875]\n",
      "Epoch: 655 , Loss: [0.4611216187477112, 0.875]\n",
      "Epoch: 656 , Loss: [0.4443856179714203, 0.87890625]\n",
      "Epoch: 657 , Loss: [0.4367431700229645, 0.8828125]\n",
      "Epoch: 658 , Loss: [0.4375242590904236, 0.89453125]\n",
      "Epoch: 659 , Loss: [0.4422873854637146, 0.875]\n",
      "Epoch: 660 , Loss: [0.4463462829589844, 0.89453125]\n",
      "Epoch: 661 , Loss: [0.4460112750530243, 0.8671875]\n",
      "Epoch: 662 , Loss: [0.46060991287231445, 0.8671875]\n",
      "Epoch: 663 , Loss: [0.42663487792015076, 0.921875]\n",
      "Epoch: 664 , Loss: [0.4376870095729828, 0.90234375]\n",
      "Epoch: 665 , Loss: [0.4515170753002167, 0.87109375]\n",
      "Epoch: 666 , Loss: [0.4223111867904663, 0.8984375]\n",
      "Epoch: 667 , Loss: [0.4355323612689972, 0.91015625]\n",
      "Epoch: 668 , Loss: [0.4250531494617462, 0.89453125]\n",
      "Epoch: 669 , Loss: [0.44001954793930054, 0.87890625]\n",
      "Epoch: 670 , Loss: [0.4158766269683838, 0.90234375]\n",
      "Epoch: 671 , Loss: [0.41173291206359863, 0.90625]\n",
      "Epoch: 672 , Loss: [0.456669420003891, 0.87109375]\n",
      "Epoch: 673 , Loss: [0.42587634921073914, 0.9140625]\n",
      "Epoch: 674 , Loss: [0.42767971754074097, 0.921875]\n",
      "Epoch: 675 , Loss: [0.44637778401374817, 0.8828125]\n",
      "Epoch: 676 , Loss: [0.44875043630599976, 0.87109375]\n",
      "Epoch: 677 , Loss: [0.436418741941452, 0.875]\n",
      "Epoch: 678 , Loss: [0.4474143385887146, 0.8671875]\n",
      "Epoch: 679 , Loss: [0.4388585686683655, 0.86328125]\n",
      "Epoch: 680 , Loss: [0.46828481554985046, 0.83984375]\n",
      "Epoch: 681 , Loss: [0.4487878978252411, 0.87109375]\n",
      "Epoch: 682 , Loss: [0.4499542713165283, 0.8671875]\n",
      "Epoch: 683 , Loss: [0.465570867061615, 0.8671875]\n",
      "Epoch: 684 , Loss: [0.4438534080982208, 0.87890625]\n",
      "Epoch: 685 , Loss: [0.40569350123405457, 0.90625]\n",
      "Epoch: 686 , Loss: [0.4485507607460022, 0.86328125]\n",
      "Epoch: 687 , Loss: [0.432451456785202, 0.8984375]\n",
      "Epoch: 688 , Loss: [0.43347981572151184, 0.89453125]\n",
      "Epoch: 689 , Loss: [0.40246838331222534, 0.90234375]\n",
      "Epoch: 690 , Loss: [0.41398170590400696, 0.89453125]\n",
      "Epoch: 691 , Loss: [0.44745326042175293, 0.88671875]\n",
      "Epoch: 692 , Loss: [0.4369240999221802, 0.890625]\n",
      "Epoch: 693 , Loss: [0.4210168719291687, 0.8984375]\n",
      "Epoch: 694 , Loss: [0.43638551235198975, 0.87109375]\n",
      "Epoch: 695 , Loss: [0.4596751034259796, 0.875]\n",
      "Epoch: 696 , Loss: [0.4263629913330078, 0.90625]\n",
      "Epoch: 697 , Loss: [0.44001689553260803, 0.8828125]\n",
      "Epoch: 698 , Loss: [0.41374725103378296, 0.9140625]\n",
      "Epoch: 699 , Loss: [0.4187805950641632, 0.90234375]\n",
      "Epoch: 700 , Loss: [0.42540213465690613, 0.86328125]\n",
      "=============================================\n",
      "18  correctly classified among  64\n",
      "Accuracy as of 700 epochs: 28.125\n",
      "=============================================\n",
      "Epoch: 701 , Loss: [0.4515644609928131, 0.8671875]\n",
      "Epoch: 702 , Loss: [0.4242079257965088, 0.90234375]\n",
      "Epoch: 703 , Loss: [0.3765089213848114, 0.921875]\n",
      "Epoch: 704 , Loss: [0.4588412046432495, 0.859375]\n",
      "Epoch: 705 , Loss: [0.4283495545387268, 0.88671875]\n",
      "Epoch: 706 , Loss: [0.4259704649448395, 0.88671875]\n",
      "Epoch: 707 , Loss: [0.4109857678413391, 0.8984375]\n",
      "Epoch: 708 , Loss: [0.42215073108673096, 0.87890625]\n",
      "Epoch: 709 , Loss: [0.4516122341156006, 0.8671875]\n",
      "Epoch: 710 , Loss: [0.43957504630088806, 0.875]\n",
      "Epoch: 711 , Loss: [0.4395974576473236, 0.88671875]\n",
      "Epoch: 712 , Loss: [0.3969750702381134, 0.890625]\n",
      "Epoch: 713 , Loss: [0.40098726749420166, 0.9140625]\n",
      "Epoch: 714 , Loss: [0.4510359466075897, 0.859375]\n",
      "Epoch: 715 , Loss: [0.41490888595581055, 0.8984375]\n",
      "Epoch: 716 , Loss: [0.4163585603237152, 0.88671875]\n",
      "Epoch: 717 , Loss: [0.42896097898483276, 0.87109375]\n",
      "Epoch: 718 , Loss: [0.4232262074947357, 0.87890625]\n",
      "Epoch: 719 , Loss: [0.40774670243263245, 0.89453125]\n",
      "Epoch: 720 , Loss: [0.44129428267478943, 0.88671875]\n",
      "Epoch: 721 , Loss: [0.4019767642021179, 0.92578125]\n",
      "Epoch: 722 , Loss: [0.41978058218955994, 0.890625]\n",
      "Epoch: 723 , Loss: [0.41667112708091736, 0.88671875]\n",
      "Epoch: 724 , Loss: [0.37104707956314087, 0.9140625]\n",
      "Epoch: 725 , Loss: [0.42212533950805664, 0.89453125]\n",
      "Epoch: 726 , Loss: [0.3947035074234009, 0.91015625]\n",
      "Epoch: 727 , Loss: [0.37896808981895447, 0.9296875]\n",
      "Epoch: 728 , Loss: [0.39942240715026855, 0.91015625]\n",
      "Epoch: 729 , Loss: [0.41952240467071533, 0.890625]\n",
      "Epoch: 730 , Loss: [0.39673325419425964, 0.92578125]\n",
      "Epoch: 731 , Loss: [0.39115792512893677, 0.921875]\n",
      "Epoch: 732 , Loss: [0.3913864493370056, 0.90625]\n",
      "Epoch: 733 , Loss: [0.38450315594673157, 0.91796875]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "n_way = 20\n",
    "n_val = 64\n",
    "batch_size = 256\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "model = get_vgg_face((224,224,3))\n",
    "\n",
    "model = Train_with_batch(model, X_train,cat_train,train_size,epochs,enc,n_way = 20,n_val = 64,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_way = 20\n",
    "n_val = 64\n",
    "batch_size = 1280\n",
    "\n",
    "# enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "model = get_vgg_face((224,224,3))\n",
    "\n",
    "batch_x, batch_y = get_batch(X_train, cat_train, train_size, batch_size)\n",
    "# batch_y = enc.fit_transform(batch_y.reshape(-1,1))\n",
    "\n",
    "history = model.fit(batch_x, batch_y,batch_size = 64,epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Face-Detection] *",
   "language": "python",
   "name": "conda-env-Face-Detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
