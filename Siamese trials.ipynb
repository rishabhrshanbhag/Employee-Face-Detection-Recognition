{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imutils import face_utils\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "cascPath = \"/Users/abdulrehman/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "def return_bbx(image):\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "    faces = faceCascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Adrien_Brody</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Alejandro_Toledo</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Alvaro_Uribe</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Amelie_Mauresmo</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>Vicente_Fox</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Vladimir_Putin</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>Wen_Jiabao</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659</th>\n",
       "      <td>Winona_Ryder</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>Yoriko_Kawaguchi</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  images\n",
       "20        Abdullah_Gul      19\n",
       "52        Adrien_Brody      12\n",
       "127   Alejandro_Toledo      39\n",
       "210       Alvaro_Uribe      35\n",
       "223    Amelie_Mauresmo      21\n",
       "...                ...     ...\n",
       "5541       Vicente_Fox      32\n",
       "5569    Vladimir_Putin      49\n",
       "5605        Wen_Jiabao      13\n",
       "5659      Winona_Ryder      24\n",
       "5704  Yoriko_Kawaguchi      14\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset_path = '/Users/abdulrehman/Desktop/SML Project/FacesInTheWild/'\n",
    "\n",
    "Celebs = pd.read_csv(Dataset_path+'lfw_allnames.csv')\n",
    "Celebs = Celebs[Celebs['images']>10]\n",
    "Celebs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = []\n",
    "X = []\n",
    "Y = []\n",
    "y_label = 0\n",
    "\n",
    "for _, [name,__] in Celebs.iterrows():\n",
    "    celeb_path = Dataset_path+'lfw-deepfunneled/'+name+'/'\n",
    "    \n",
    "    images_paths = get_files(celeb_path)\n",
    "    temp = []\n",
    "    for image_path in images_paths:\n",
    "        image = cv2.imread(celeb_path+image_path,1)\n",
    "        faces = return_bbx(image)\n",
    "        if len(faces) == 1:\n",
    "            if len(temp)>=10:\n",
    "                break\n",
    "            temp.append(len(X))\n",
    "            (x,y,w,h) = faces[0]\n",
    "            cropped = image[x:x+w, y:y+h]\n",
    "            dim = (100, 100)\n",
    "            resized = cv2.resize(cropped, dim, interpolation = cv2.INTER_AREA)\n",
    "            image = np.array(resized).astype(\"float32\")\n",
    "            X.append(image)\n",
    "            Y.append(y_label)\n",
    "    y_label+=1\n",
    "    cat_list.append(temp)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1423, 100, 100, 3) (1423,) (143,)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.asarray(X)/255\n",
    "Y_data = np.array(Y)\n",
    "cat_list = np.asarray(cat_list)\n",
    "\n",
    "print(X_data.shape, Y_data.shape, cat_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 10,\n",
       "         1: 10,\n",
       "         2: 10,\n",
       "         3: 10,\n",
       "         4: 10,\n",
       "         5: 10,\n",
       "         6: 10,\n",
       "         7: 10,\n",
       "         8: 10,\n",
       "         9: 10,\n",
       "         10: 10,\n",
       "         11: 10,\n",
       "         12: 10,\n",
       "         13: 10,\n",
       "         14: 10,\n",
       "         15: 10,\n",
       "         16: 10,\n",
       "         17: 10,\n",
       "         18: 10,\n",
       "         19: 10,\n",
       "         20: 10,\n",
       "         21: 10,\n",
       "         22: 10,\n",
       "         23: 10,\n",
       "         24: 10,\n",
       "         25: 10,\n",
       "         26: 10,\n",
       "         27: 10,\n",
       "         28: 10,\n",
       "         29: 10,\n",
       "         30: 10,\n",
       "         31: 10,\n",
       "         32: 10,\n",
       "         33: 10,\n",
       "         34: 10,\n",
       "         35: 10,\n",
       "         36: 10,\n",
       "         37: 9,\n",
       "         38: 10,\n",
       "         39: 10,\n",
       "         40: 10,\n",
       "         41: 10,\n",
       "         42: 10,\n",
       "         43: 10,\n",
       "         44: 10,\n",
       "         45: 10,\n",
       "         46: 10,\n",
       "         47: 10,\n",
       "         48: 10,\n",
       "         49: 10,\n",
       "         50: 10,\n",
       "         51: 10,\n",
       "         52: 10,\n",
       "         53: 10,\n",
       "         54: 10,\n",
       "         55: 10,\n",
       "         56: 10,\n",
       "         57: 10,\n",
       "         58: 10,\n",
       "         59: 10,\n",
       "         60: 10,\n",
       "         61: 10,\n",
       "         62: 10,\n",
       "         63: 10,\n",
       "         64: 10,\n",
       "         65: 10,\n",
       "         66: 10,\n",
       "         67: 10,\n",
       "         68: 10,\n",
       "         69: 10,\n",
       "         70: 10,\n",
       "         71: 10,\n",
       "         72: 10,\n",
       "         73: 10,\n",
       "         74: 10,\n",
       "         75: 10,\n",
       "         76: 10,\n",
       "         77: 10,\n",
       "         78: 10,\n",
       "         79: 10,\n",
       "         80: 10,\n",
       "         81: 10,\n",
       "         82: 10,\n",
       "         83: 10,\n",
       "         84: 10,\n",
       "         85: 10,\n",
       "         86: 10,\n",
       "         87: 10,\n",
       "         88: 10,\n",
       "         89: 10,\n",
       "         90: 10,\n",
       "         91: 10,\n",
       "         92: 10,\n",
       "         93: 10,\n",
       "         94: 10,\n",
       "         95: 10,\n",
       "         96: 10,\n",
       "         97: 10,\n",
       "         98: 10,\n",
       "         99: 10,\n",
       "         100: 10,\n",
       "         101: 10,\n",
       "         102: 10,\n",
       "         103: 8,\n",
       "         104: 10,\n",
       "         105: 10,\n",
       "         106: 10,\n",
       "         107: 9,\n",
       "         108: 10,\n",
       "         109: 10,\n",
       "         110: 10,\n",
       "         111: 10,\n",
       "         112: 10,\n",
       "         113: 10,\n",
       "         114: 10,\n",
       "         115: 8,\n",
       "         116: 10,\n",
       "         117: 10,\n",
       "         118: 10,\n",
       "         119: 10,\n",
       "         120: 10,\n",
       "         121: 10,\n",
       "         122: 10,\n",
       "         123: 10,\n",
       "         124: 9,\n",
       "         125: 10,\n",
       "         126: 10,\n",
       "         127: 10,\n",
       "         128: 10,\n",
       "         129: 10,\n",
       "         130: 10,\n",
       "         131: 10,\n",
       "         132: 10,\n",
       "         133: 10,\n",
       "         134: 10,\n",
       "         135: 10,\n",
       "         136: 10,\n",
       "         137: 10,\n",
       "         138: 10,\n",
       "         139: 10,\n",
       "         140: 10,\n",
       "         141: 10,\n",
       "         142: 10})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = Y_data\n",
    "n_classes = len(set(a))\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X&Y shape of training data : (1280, 100, 100, 3) and (1280,) (128,)\n",
      "X&Y shape of testing data : (143, 100, 100, 3) and (143,) (15,)\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "\n",
    "train_size = int(n_classes*train_split)\n",
    "test_size = n_classes-train_size\n",
    "\n",
    "train_files = train_size * 10\n",
    "\n",
    "X_train = X_data[:train_files]\n",
    "y_train = Y_data[:train_files]\n",
    "cat_train = cat_list[:train_size]\n",
    "\n",
    "#Validation Split\n",
    "X_test = X_data[train_files:]\n",
    "y_test = Y_data[train_files:]\n",
    "cat_test = cat_list[train_size:]\n",
    "\n",
    "print('X&Y shape of training data :',X_train.shape, 'and', y_train.shape, cat_train.shape)\n",
    "print('X&Y shape of testing data :' , X_test.shape, 'and', y_test.shape, cat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=64):\n",
    "    \n",
    "    temp_x = X_train\n",
    "    temp_cat_list = cat_train\n",
    "    start=0\n",
    "    end=train_size\n",
    "    batch_x=[]\n",
    "        \n",
    "    batch_y = np.zeros(batch_size)\n",
    "    batch_y[int(batch_size/2):] = 1\n",
    "    np.random.shuffle(batch_y)\n",
    "    \n",
    "    class_list = np.random.randint(start, end, batch_size) \n",
    "    batch_x.append(np.zeros((batch_size, 100, 100, 3)))\n",
    "    batch_x.append(np.zeros((batch_size, 100, 100, 3)))\n",
    "\n",
    "    for i in range(0, batch_size):\n",
    "        batch_x[0][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]  \n",
    "        #If train_y has 0 pick from the same class, else pick from any other class\n",
    "        if batch_y[i]==0:\n",
    "            batch_x[1][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
    "\n",
    "        else:\n",
    "            temp_list = np.append(temp_cat_list[:class_list[i]].flatten(), temp_cat_list[class_list[i]+1:].flatten())\n",
    "            temp_list = np.random.choice(temp_list)\n",
    "            batch_x[1][i] = temp_x[np.random.choice(temp_list)]\n",
    "            \n",
    "            \n",
    "    return(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input_7 (InputLayer)       [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input_8 (InputLayer)      [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "left_block1_conv1 (Conv2D)      (None, 100, 100, 64) 1792        left_input_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "right_block1_conv1 (Conv2D)     (None, 100, 100, 64) 1792        right_input_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "left_block1_conv2 (Conv2D)      (None, 100, 100, 64) 36928       left_block1_conv1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block1_conv2 (Conv2D)     (None, 100, 100, 64) 36928       right_block1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block1_pool (MaxPooling2D) (None, 50, 50, 64)   0           left_block1_conv2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block1_pool (MaxPooling2D (None, 50, 50, 64)   0           right_block1_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block2_conv1 (Conv2D)      (None, 50, 50, 128)  73856       left_block1_pool[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "right_block2_conv1 (Conv2D)     (None, 50, 50, 128)  73856       right_block1_pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "left_block2_conv2 (Conv2D)      (None, 50, 50, 128)  147584      left_block2_conv1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block2_conv2 (Conv2D)     (None, 50, 50, 128)  147584      right_block2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block2_pool (MaxPooling2D) (None, 25, 25, 128)  0           left_block2_conv2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block2_pool (MaxPooling2D (None, 25, 25, 128)  0           right_block2_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block3_conv1 (Conv2D)      (None, 25, 25, 256)  295168      left_block2_pool[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "right_block3_conv1 (Conv2D)     (None, 25, 25, 256)  295168      right_block2_pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "left_block3_conv2 (Conv2D)      (None, 25, 25, 256)  590080      left_block3_conv1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block3_conv2 (Conv2D)     (None, 25, 25, 256)  590080      right_block3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block3_conv3 (Conv2D)      (None, 25, 25, 256)  590080      left_block3_conv2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block3_conv3 (Conv2D)     (None, 25, 25, 256)  590080      right_block3_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block3_pool (MaxPooling2D) (None, 12, 12, 256)  0           left_block3_conv3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block3_pool (MaxPooling2D (None, 12, 12, 256)  0           right_block3_conv3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block4_conv1 (Conv2D)      (None, 12, 12, 512)  1180160     left_block3_pool[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "right_block4_conv1 (Conv2D)     (None, 12, 12, 512)  1180160     right_block3_pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "left_block4_conv2 (Conv2D)      (None, 12, 12, 512)  2359808     left_block4_conv1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block4_conv2 (Conv2D)     (None, 12, 12, 512)  2359808     right_block4_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block4_conv3 (Conv2D)      (None, 12, 12, 512)  2359808     left_block4_conv2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block4_conv3 (Conv2D)     (None, 12, 12, 512)  2359808     right_block4_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "left_block4_pool (MaxPooling2D) (None, 6, 6, 512)    0           left_block4_conv3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "right_block4_pool (MaxPooling2D (None, 6, 6, 512)    0           right_block4_conv3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 18432)        0           left_block4_pool[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 18432)        0           right_block4_pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4096)         75501568    flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 4096)         75501568    flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4096)         16384       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4096)         16384       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4096)         0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 4096)         0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 4096)         0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            4097        subtract_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 166,310,529\n",
      "Trainable params: 151,023,617\n",
      "Non-trainable params: 15,286,912\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Subtract, Input, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "input_shape = (100,100,3)\n",
    "\n",
    "vgg_left = VGG16(weights = 'imagenet',include_top = False, input_shape = input_shape)\n",
    "\n",
    "for layer in vgg_left.layers:\n",
    "    layer.trainable = False\n",
    "    layer._name = 'left_'+layer.name\n",
    "    \n",
    "left = [layer.output for layer in vgg_left.layers][-5]\n",
    "\n",
    "left = Flatten()(left)\n",
    "# left = Dropout(0.5)(left)\n",
    "left = Dense(4096, kernel_regularizer=l2(1e-2))(left)\n",
    "left = BatchNormalization()(left)\n",
    "left = Activation('sigmoid')(left)\n",
    "\n",
    "\n",
    "vgg_right = VGG16(weights = 'imagenet',include_top = False, input_shape = input_shape)\n",
    "\n",
    "for layer in vgg_right.layers:\n",
    "    layer.trainable = False\n",
    "    layer._name = 'right_'+layer.name\n",
    "\n",
    "right = [layer.output for layer in vgg_right.layers][-5]\n",
    "\n",
    "right = Flatten()(right)\n",
    "# right = Dropout(0.5)(right)\n",
    "right = Dense(4096, kernel_regularizer=l2(1e-2))(right)\n",
    "right = BatchNormalization()(right)\n",
    "right = Activation('sigmoid')(right)\n",
    "\n",
    "\n",
    "subtracted = Subtract()([left,right])\n",
    "# subtracted = Dense(128, activation='sigmoid')(subtracted)\n",
    "out = Dense(1, activation='sigmoid')(subtracted)\n",
    "\n",
    "model = Model(inputs = [vgg_left.input,vgg_right.input], outputs = out)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications import VGG16\n",
    "# from keras.layers import Input, Dense, Flatten, Subtract, Conv2D, MaxPooling2D\n",
    "# from keras.layers import Dropout, Activation, BatchNormalization\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2\n",
    "# from keras.utils import plot_model\n",
    "# # from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "# input_shape = (100,100,3)\n",
    "\n",
    "# left_input = Input(input_shape)\n",
    "# right_input = Input(input_shape)\n",
    "\n",
    "# left = Sequential()\n",
    "# left.add(left_input)\n",
    "# left.add(Conv2D(64, (3,3), activation='relu'))\n",
    "# left.add(MaxPooling2D(2,2))\n",
    "# left.add(Conv2D(128, (3,3), activation='relu'))\n",
    "# left.add(MaxPooling2D(2,2))\n",
    "# left.add(Conv2D(128, (3,3), activation='relu'))\n",
    "# left.add(MaxPooling2D(2,2))\n",
    "# left.add(Conv2D(256, (3,3), activation='relu'))\n",
    "# left.add(MaxPooling2D(2,2))\n",
    "# left.add(Flatten())\n",
    "# left.add(Dense(1028, activation='relu', kernel_regularizer=l2(1e-2)))\n",
    "\n",
    "# right = Sequential()\n",
    "# right.add(right_input)\n",
    "# right.add(Conv2D(64, (3,3), activation='relu'))\n",
    "# right.add(MaxPooling2D(2,2))\n",
    "# right.add(Conv2D(128, (3,3), activation='relu'))\n",
    "# right.add(MaxPooling2D(2,2))\n",
    "# right.add(Conv2D(128, (3,3), activation='relu'))\n",
    "# right.add(MaxPooling2D(2,2))\n",
    "# right.add(Conv2D(256, (3,3), activation='relu'))\n",
    "# right.add(MaxPooling2D(2,2))\n",
    "# right.add(Flatten())\n",
    "# right.add(Dense(1028, activation='relu', kernel_regularizer=l2(1e-2)))\n",
    "\n",
    "# subtracted = Subtract()([left.output,right.output])\n",
    "# subtracted = Dense(128, activation='sigmoid')(subtracted)\n",
    "# out = Dense(1, activation='sigmoid')(subtracted)\n",
    "\n",
    "# model = Model(inputs = [left.input, right.input], outputs = out)\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_learning(model, n_way, n_val):\n",
    "    \n",
    "    temp_x = X_test\n",
    "    temp_cat_list = cat_test\n",
    "    batch_x=[]\n",
    "    x_0_choice=[]\n",
    "    n_correct = 0\n",
    "   \n",
    "    class_list = np.random.randint(train_size+1, n_classes-1, n_val)\n",
    "\n",
    "    for i in class_list:  \n",
    "        j = np.random.choice(cat_list[i])\n",
    "        temp=[]\n",
    "        temp.append(np.zeros((n_way, 100, 100, 3)))\n",
    "        temp.append(np.zeros((n_way, 100, 100, 3)))\n",
    "        for k in range(0, n_way):\n",
    "            temp[0][k] = X_data[j]\n",
    "            \n",
    "            if k==0:\n",
    "                temp[1][k] = X_data[np.random.choice(cat_list[i])]\n",
    "            else:\n",
    "                temp_list = np.append(cat_list[:i].flatten(), cat_list[i+1:].flatten())\n",
    "                temp_list = np.random.choice(temp_list)\n",
    "                temp[1][k] = X_data[np.random.choice(temp_list)]\n",
    "\n",
    "        result = model.predict(temp)\n",
    "        result = result.flatten().tolist()\n",
    "        result_index = result.index(min(result))\n",
    "        if result_index == 0:\n",
    "            n_correct = n_correct + 1\n",
    "    print(n_correct, \"correctly classified among\", n_val)\n",
    "    accuracy = (n_correct*100)/n_val\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: [134.71145629882812, 0.578125]\n",
      "Epoch: 2 , Loss: [132.87075805664062, 0.5]\n",
      "Epoch: 3 , Loss: [131.03445434570312, 0.484375]\n",
      "Epoch: 4 , Loss: [129.51890563964844, 0.375]\n",
      "Epoch: 5 , Loss: [127.78551483154297, 0.453125]\n",
      "Epoch: 6 , Loss: [125.80372619628906, 0.46875]\n",
      "Epoch: 7 , Loss: [123.91171264648438, 0.515625]\n",
      "Epoch: 8 , Loss: [122.10346984863281, 0.578125]\n",
      "Epoch: 9 , Loss: [120.44327545166016, 0.546875]\n",
      "Epoch: 10 , Loss: [119.02418518066406, 0.390625]\n",
      "Epoch: 11 , Loss: [117.15896606445312, 0.5625]\n",
      "Epoch: 12 , Loss: [115.63201904296875, 0.421875]\n",
      "Epoch: 13 , Loss: [113.78555297851562, 0.484375]\n",
      "Epoch: 14 , Loss: [112.2431869506836, 0.515625]\n",
      "Epoch: 15 , Loss: [110.67958068847656, 0.53125]\n",
      "Epoch: 16 , Loss: [108.74568176269531, 0.484375]\n",
      "Epoch: 17 , Loss: [107.40313720703125, 0.53125]\n",
      "Epoch: 18 , Loss: [105.84418487548828, 0.453125]\n",
      "Epoch: 19 , Loss: [105.04824829101562, 0.390625]\n",
      "Epoch: 20 , Loss: [103.12879180908203, 0.46875]\n",
      "Epoch: 21 , Loss: [101.14493560791016, 0.609375]\n",
      "Epoch: 22 , Loss: [99.89389038085938, 0.453125]\n",
      "Epoch: 23 , Loss: [98.92996215820312, 0.40625]\n",
      "Epoch: 24 , Loss: [96.9224853515625, 0.609375]\n",
      "Epoch: 25 , Loss: [95.9120101928711, 0.390625]\n",
      "Epoch: 26 , Loss: [94.63214111328125, 0.4375]\n",
      "Epoch: 27 , Loss: [93.08512878417969, 0.421875]\n",
      "Epoch: 28 , Loss: [91.62818908691406, 0.5]\n",
      "Epoch: 29 , Loss: [90.00883483886719, 0.546875]\n",
      "Epoch: 30 , Loss: [89.00255584716797, 0.53125]\n",
      "Epoch: 31 , Loss: [87.5753173828125, 0.578125]\n",
      "Epoch: 32 , Loss: [86.67308044433594, 0.390625]\n",
      "Epoch: 33 , Loss: [85.04315948486328, 0.546875]\n",
      "Epoch: 34 , Loss: [84.10315704345703, 0.484375]\n",
      "Epoch: 35 , Loss: [82.4298095703125, 0.578125]\n",
      "Epoch: 36 , Loss: [81.55029296875, 0.46875]\n",
      "Epoch: 37 , Loss: [80.78545379638672, 0.390625]\n",
      "Epoch: 38 , Loss: [79.18778228759766, 0.5]\n",
      "Epoch: 39 , Loss: [78.08624267578125, 0.421875]\n",
      "Epoch: 40 , Loss: [76.81780242919922, 0.5]\n",
      "Epoch: 41 , Loss: [75.62226867675781, 0.546875]\n",
      "Epoch: 42 , Loss: [74.75370788574219, 0.46875]\n",
      "Epoch: 43 , Loss: [73.55244445800781, 0.4375]\n",
      "Epoch: 44 , Loss: [72.5707778930664, 0.484375]\n",
      "Epoch: 45 , Loss: [71.42788696289062, 0.46875]\n",
      "Epoch: 46 , Loss: [70.72030639648438, 0.4375]\n",
      "Epoch: 47 , Loss: [70.12602233886719, 0.375]\n",
      "Epoch: 48 , Loss: [68.53864288330078, 0.53125]\n",
      "Epoch: 49 , Loss: [67.45269775390625, 0.484375]\n",
      "Epoch: 50 , Loss: [66.56918334960938, 0.546875]\n",
      "Epoch: 51 , Loss: [65.61802673339844, 0.4375]\n",
      "Epoch: 52 , Loss: [64.57537841796875, 0.5]\n",
      "Epoch: 53 , Loss: [63.851600646972656, 0.59375]\n",
      "Epoch: 54 , Loss: [62.88206481933594, 0.53125]\n",
      "Epoch: 55 , Loss: [62.01251220703125, 0.484375]\n",
      "Epoch: 56 , Loss: [60.980045318603516, 0.5]\n",
      "Epoch: 57 , Loss: [59.78704833984375, 0.625]\n",
      "Epoch: 58 , Loss: [59.369415283203125, 0.421875]\n",
      "Epoch: 59 , Loss: [58.5054931640625, 0.46875]\n",
      "Epoch: 60 , Loss: [57.5736198425293, 0.515625]\n",
      "Epoch: 61 , Loss: [56.722900390625, 0.421875]\n",
      "Epoch: 62 , Loss: [55.93128204345703, 0.5]\n",
      "Epoch: 63 , Loss: [55.04931640625, 0.515625]\n",
      "Epoch: 64 , Loss: [54.18967056274414, 0.53125]\n",
      "Epoch: 65 , Loss: [53.5010871887207, 0.546875]\n",
      "Epoch: 66 , Loss: [53.26594161987305, 0.390625]\n",
      "Epoch: 67 , Loss: [52.54498291015625, 0.359375]\n",
      "Epoch: 68 , Loss: [51.16252136230469, 0.53125]\n",
      "Epoch: 69 , Loss: [50.69910430908203, 0.453125]\n",
      "Epoch: 70 , Loss: [50.102657318115234, 0.484375]\n",
      "Epoch: 71 , Loss: [49.23388671875, 0.484375]\n",
      "Epoch: 72 , Loss: [48.50957489013672, 0.46875]\n",
      "Epoch: 73 , Loss: [48.06827926635742, 0.375]\n",
      "Epoch: 74 , Loss: [46.93671417236328, 0.578125]\n",
      "Epoch: 75 , Loss: [46.418121337890625, 0.453125]\n",
      "Epoch: 76 , Loss: [45.68596649169922, 0.4375]\n",
      "Epoch: 77 , Loss: [45.1083984375, 0.484375]\n",
      "Epoch: 78 , Loss: [44.53239059448242, 0.390625]\n",
      "Epoch: 79 , Loss: [43.78883743286133, 0.4375]\n",
      "Epoch: 80 , Loss: [43.051734924316406, 0.53125]\n",
      "Epoch: 81 , Loss: [42.58306884765625, 0.53125]\n",
      "Epoch: 82 , Loss: [42.141815185546875, 0.421875]\n",
      "Epoch: 83 , Loss: [41.529422760009766, 0.484375]\n",
      "Epoch: 84 , Loss: [40.87348937988281, 0.546875]\n",
      "Epoch: 85 , Loss: [40.28019714355469, 0.53125]\n",
      "Epoch: 86 , Loss: [39.61298370361328, 0.5]\n",
      "Epoch: 87 , Loss: [39.057861328125, 0.5625]\n",
      "Epoch: 88 , Loss: [38.45063781738281, 0.5625]\n",
      "Epoch: 89 , Loss: [38.12147521972656, 0.53125]\n",
      "Epoch: 90 , Loss: [37.62449645996094, 0.5]\n",
      "Epoch: 91 , Loss: [36.99718475341797, 0.421875]\n",
      "Epoch: 92 , Loss: [36.419918060302734, 0.5625]\n",
      "Epoch: 93 , Loss: [35.86552047729492, 0.515625]\n",
      "Epoch: 94 , Loss: [35.44929504394531, 0.609375]\n",
      "Epoch: 95 , Loss: [35.327552795410156, 0.484375]\n",
      "Epoch: 96 , Loss: [34.521751403808594, 0.53125]\n",
      "Epoch: 97 , Loss: [34.22124481201172, 0.53125]\n",
      "Epoch: 98 , Loss: [33.478851318359375, 0.515625]\n",
      "Epoch: 99 , Loss: [33.390586853027344, 0.4375]\n",
      "Epoch: 100 , Loss: [32.776039123535156, 0.40625]\n",
      "Epoch: 101 , Loss: [32.33282470703125, 0.4375]\n",
      "Epoch: 102 , Loss: [31.874618530273438, 0.46875]\n",
      "Epoch: 103 , Loss: [31.336376190185547, 0.453125]\n",
      "Epoch: 104 , Loss: [31.014888763427734, 0.46875]\n",
      "Epoch: 105 , Loss: [30.4507999420166, 0.515625]\n",
      "Epoch: 106 , Loss: [29.877490997314453, 0.5625]\n",
      "Epoch: 107 , Loss: [29.631723403930664, 0.5]\n",
      "Epoch: 108 , Loss: [29.28749656677246, 0.46875]\n",
      "Epoch: 109 , Loss: [28.860071182250977, 0.4375]\n",
      "Epoch: 110 , Loss: [28.792274475097656, 0.421875]\n",
      "Epoch: 111 , Loss: [27.946584701538086, 0.609375]\n",
      "Epoch: 112 , Loss: [27.850379943847656, 0.40625]\n",
      "Epoch: 113 , Loss: [27.259822845458984, 0.546875]\n",
      "Epoch: 114 , Loss: [27.390304565429688, 0.28125]\n",
      "Epoch: 115 , Loss: [26.655141830444336, 0.46875]\n",
      "Epoch: 116 , Loss: [26.278329849243164, 0.515625]\n",
      "Epoch: 117 , Loss: [25.890045166015625, 0.484375]\n",
      "Epoch: 118 , Loss: [25.5439453125, 0.484375]\n",
      "Epoch: 119 , Loss: [25.32447052001953, 0.484375]\n",
      "Epoch: 120 , Loss: [24.85270881652832, 0.5]\n",
      "Epoch: 121 , Loss: [24.528484344482422, 0.40625]\n",
      "Epoch: 122 , Loss: [23.971364974975586, 0.609375]\n",
      "Epoch: 123 , Loss: [23.98696517944336, 0.4375]\n",
      "Epoch: 124 , Loss: [23.352224349975586, 0.59375]\n",
      "Epoch: 125 , Loss: [23.34075355529785, 0.421875]\n",
      "Epoch: 126 , Loss: [23.13306427001953, 0.4375]\n",
      "Epoch: 127 , Loss: [22.432815551757812, 0.59375]\n",
      "Epoch: 128 , Loss: [22.519906997680664, 0.40625]\n",
      "Epoch: 129 , Loss: [22.10978889465332, 0.53125]\n",
      "Epoch: 130 , Loss: [21.79534149169922, 0.59375]\n",
      "Epoch: 131 , Loss: [21.33201789855957, 0.53125]\n",
      "Epoch: 132 , Loss: [21.294937133789062, 0.484375]\n",
      "Epoch: 133 , Loss: [21.021686553955078, 0.4375]\n",
      "Epoch: 134 , Loss: [20.560535430908203, 0.578125]\n",
      "Epoch: 135 , Loss: [20.509004592895508, 0.484375]\n",
      "Epoch: 136 , Loss: [20.33027458190918, 0.359375]\n",
      "Epoch: 137 , Loss: [19.847244262695312, 0.5625]\n",
      "Epoch: 138 , Loss: [19.86663055419922, 0.484375]\n",
      "Epoch: 139 , Loss: [19.345834732055664, 0.515625]\n",
      "Epoch: 140 , Loss: [19.211416244506836, 0.40625]\n",
      "Epoch: 141 , Loss: [19.025415420532227, 0.5]\n",
      "Epoch: 142 , Loss: [18.6597843170166, 0.484375]\n",
      "Epoch: 143 , Loss: [18.481746673583984, 0.5]\n",
      "Epoch: 144 , Loss: [18.145761489868164, 0.4375]\n",
      "Epoch: 145 , Loss: [18.072589874267578, 0.40625]\n",
      "Epoch: 146 , Loss: [17.730350494384766, 0.40625]\n",
      "Epoch: 147 , Loss: [17.551063537597656, 0.5]\n",
      "Epoch: 148 , Loss: [17.2614688873291, 0.53125]\n",
      "Epoch: 149 , Loss: [16.931777954101562, 0.53125]\n",
      "Epoch: 150 , Loss: [16.745851516723633, 0.515625]\n",
      "Epoch: 151 , Loss: [16.604007720947266, 0.546875]\n",
      "Epoch: 152 , Loss: [16.586563110351562, 0.421875]\n",
      "Epoch: 153 , Loss: [16.214099884033203, 0.4375]\n",
      "Epoch: 154 , Loss: [16.06786346435547, 0.515625]\n",
      "Epoch: 155 , Loss: [15.907310485839844, 0.5]\n",
      "Epoch: 156 , Loss: [15.875247955322266, 0.390625]\n",
      "Epoch: 157 , Loss: [15.476293563842773, 0.5]\n",
      "Epoch: 158 , Loss: [15.121503829956055, 0.578125]\n",
      "Epoch: 159 , Loss: [15.215184211730957, 0.46875]\n",
      "Epoch: 160 , Loss: [14.794500350952148, 0.53125]\n",
      "Epoch: 161 , Loss: [14.489701271057129, 0.671875]\n",
      "Epoch: 162 , Loss: [14.470060348510742, 0.578125]\n",
      "Epoch: 163 , Loss: [14.609655380249023, 0.5]\n",
      "Epoch: 164 , Loss: [14.085468292236328, 0.578125]\n",
      "Epoch: 165 , Loss: [13.86047649383545, 0.625]\n",
      "Epoch: 166 , Loss: [14.045568466186523, 0.390625]\n",
      "Epoch: 167 , Loss: [13.715683937072754, 0.46875]\n",
      "Epoch: 168 , Loss: [13.570676803588867, 0.515625]\n",
      "Epoch: 169 , Loss: [13.207636833190918, 0.640625]\n",
      "Epoch: 170 , Loss: [13.355500221252441, 0.390625]\n",
      "Epoch: 171 , Loss: [13.303571701049805, 0.4375]\n",
      "Epoch: 172 , Loss: [12.906867980957031, 0.578125]\n",
      "Epoch: 173 , Loss: [12.646756172180176, 0.53125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174 , Loss: [12.617393493652344, 0.546875]\n",
      "Epoch: 175 , Loss: [12.500940322875977, 0.46875]\n",
      "Epoch: 176 , Loss: [12.308443069458008, 0.53125]\n",
      "Epoch: 177 , Loss: [12.117708206176758, 0.578125]\n",
      "Epoch: 178 , Loss: [12.043132781982422, 0.53125]\n",
      "Epoch: 179 , Loss: [11.943619728088379, 0.5]\n",
      "Epoch: 180 , Loss: [11.882610321044922, 0.515625]\n",
      "Epoch: 181 , Loss: [11.633068084716797, 0.515625]\n",
      "Epoch: 182 , Loss: [11.589685440063477, 0.46875]\n",
      "Epoch: 183 , Loss: [11.479053497314453, 0.453125]\n",
      "Epoch: 184 , Loss: [11.204776763916016, 0.5625]\n",
      "Epoch: 185 , Loss: [11.063711166381836, 0.5625]\n",
      "Epoch: 186 , Loss: [10.819282531738281, 0.625]\n",
      "Epoch: 187 , Loss: [10.7879638671875, 0.484375]\n",
      "Epoch: 188 , Loss: [10.78528118133545, 0.375]\n",
      "Epoch: 189 , Loss: [10.550376892089844, 0.484375]\n",
      "Epoch: 190 , Loss: [10.54350757598877, 0.421875]\n",
      "Epoch: 191 , Loss: [10.351387023925781, 0.5]\n",
      "Epoch: 192 , Loss: [10.321025848388672, 0.46875]\n",
      "Epoch: 193 , Loss: [10.005826950073242, 0.546875]\n",
      "Epoch: 194 , Loss: [10.17524528503418, 0.46875]\n",
      "Epoch: 195 , Loss: [9.795327186584473, 0.5625]\n",
      "Epoch: 196 , Loss: [9.962413787841797, 0.40625]\n",
      "Epoch: 197 , Loss: [9.610926628112793, 0.53125]\n",
      "Epoch: 198 , Loss: [9.502641677856445, 0.5]\n",
      "Epoch: 199 , Loss: [9.47104263305664, 0.515625]\n",
      "Epoch: 200 , Loss: [9.383838653564453, 0.4375]\n",
      "Epoch: 201 , Loss: [9.282974243164062, 0.53125]\n",
      "Epoch: 202 , Loss: [9.016040802001953, 0.65625]\n",
      "Epoch: 203 , Loss: [9.092811584472656, 0.484375]\n",
      "Epoch: 204 , Loss: [8.91634750366211, 0.625]\n",
      "Epoch: 205 , Loss: [9.206220626831055, 0.3125]\n",
      "Epoch: 206 , Loss: [8.747377395629883, 0.578125]\n",
      "Epoch: 207 , Loss: [8.863128662109375, 0.421875]\n",
      "Epoch: 208 , Loss: [8.686415672302246, 0.453125]\n",
      "Epoch: 209 , Loss: [8.592719078063965, 0.421875]\n",
      "Epoch: 210 , Loss: [8.371245384216309, 0.59375]\n",
      "Epoch: 211 , Loss: [8.38244915008545, 0.453125]\n",
      "Epoch: 212 , Loss: [8.305299758911133, 0.546875]\n",
      "Epoch: 213 , Loss: [8.147315979003906, 0.46875]\n",
      "Epoch: 214 , Loss: [8.123607635498047, 0.5]\n",
      "Epoch: 215 , Loss: [7.9676971435546875, 0.578125]\n",
      "Epoch: 216 , Loss: [7.908201217651367, 0.515625]\n",
      "Epoch: 217 , Loss: [7.7516632080078125, 0.609375]\n",
      "Epoch: 218 , Loss: [7.814420223236084, 0.5]\n",
      "Epoch: 219 , Loss: [7.766302108764648, 0.46875]\n",
      "Epoch: 220 , Loss: [7.484559059143066, 0.578125]\n",
      "Epoch: 221 , Loss: [7.545689105987549, 0.46875]\n",
      "Epoch: 222 , Loss: [7.395033836364746, 0.609375]\n",
      "Epoch: 223 , Loss: [7.309617519378662, 0.515625]\n",
      "Epoch: 224 , Loss: [7.3757004737854, 0.484375]\n",
      "Epoch: 225 , Loss: [7.103522777557373, 0.578125]\n",
      "Epoch: 226 , Loss: [7.195919036865234, 0.46875]\n",
      "Epoch: 227 , Loss: [7.116103172302246, 0.5]\n",
      "Epoch: 228 , Loss: [7.118380546569824, 0.46875]\n",
      "Epoch: 229 , Loss: [6.896672248840332, 0.546875]\n",
      "Epoch: 230 , Loss: [6.848413467407227, 0.546875]\n",
      "Epoch: 231 , Loss: [6.676961898803711, 0.5625]\n",
      "Epoch: 232 , Loss: [6.673026084899902, 0.53125]\n",
      "Epoch: 233 , Loss: [6.550850868225098, 0.625]\n",
      "Epoch: 234 , Loss: [6.653688430786133, 0.578125]\n",
      "Epoch: 235 , Loss: [6.705370903015137, 0.46875]\n",
      "Epoch: 236 , Loss: [6.6686882972717285, 0.390625]\n",
      "Epoch: 237 , Loss: [6.526933193206787, 0.484375]\n",
      "Epoch: 238 , Loss: [6.320985794067383, 0.53125]\n",
      "Epoch: 239 , Loss: [6.155365943908691, 0.5625]\n",
      "Epoch: 240 , Loss: [6.373332977294922, 0.40625]\n",
      "Epoch: 241 , Loss: [6.147574424743652, 0.53125]\n",
      "Epoch: 242 , Loss: [6.215364933013916, 0.421875]\n",
      "Epoch: 243 , Loss: [6.04083251953125, 0.5625]\n",
      "Epoch: 244 , Loss: [6.031861782073975, 0.46875]\n",
      "Epoch: 245 , Loss: [5.986123085021973, 0.484375]\n",
      "Epoch: 246 , Loss: [5.844550609588623, 0.59375]\n",
      "Epoch: 247 , Loss: [5.889232635498047, 0.4375]\n",
      "Epoch: 248 , Loss: [5.841352462768555, 0.515625]\n",
      "Epoch: 249 , Loss: [5.908726692199707, 0.421875]\n",
      "Epoch: 250 , Loss: [5.60638427734375, 0.5625]\n",
      "=============================================\n",
      "0 correctly classified among 64\n",
      "Accuracy as of 250 epochs: 0.0\n",
      "=============================================\n",
      "Epoch: 251 , Loss: [5.716046333312988, 0.5]\n",
      "Epoch: 252 , Loss: [5.534736156463623, 0.5625]\n",
      "Epoch: 253 , Loss: [5.507291793823242, 0.59375]\n",
      "Epoch: 254 , Loss: [5.572896957397461, 0.5]\n",
      "Epoch: 255 , Loss: [5.511674404144287, 0.390625]\n",
      "Epoch: 256 , Loss: [5.312074661254883, 0.5]\n",
      "Epoch: 257 , Loss: [5.5875701904296875, 0.4375]\n",
      "Epoch: 258 , Loss: [5.383955955505371, 0.515625]\n",
      "Epoch: 259 , Loss: [5.314955711364746, 0.46875]\n",
      "Epoch: 260 , Loss: [5.330644607543945, 0.453125]\n",
      "Epoch: 261 , Loss: [5.297656059265137, 0.40625]\n",
      "Epoch: 262 , Loss: [5.133347511291504, 0.5]\n",
      "Epoch: 263 , Loss: [4.926013469696045, 0.5625]\n",
      "Epoch: 264 , Loss: [5.061943054199219, 0.5]\n",
      "Epoch: 265 , Loss: [5.047989845275879, 0.46875]\n",
      "Epoch: 266 , Loss: [4.89328670501709, 0.46875]\n",
      "Epoch: 267 , Loss: [4.922883033752441, 0.53125]\n",
      "Epoch: 268 , Loss: [4.820534706115723, 0.546875]\n",
      "Epoch: 269 , Loss: [4.890832901000977, 0.484375]\n",
      "Epoch: 270 , Loss: [4.746308326721191, 0.578125]\n",
      "Epoch: 271 , Loss: [4.610337257385254, 0.640625]\n",
      "Epoch: 272 , Loss: [4.698957443237305, 0.53125]\n",
      "Epoch: 273 , Loss: [4.852524280548096, 0.453125]\n",
      "Epoch: 274 , Loss: [4.680894374847412, 0.5]\n",
      "Epoch: 275 , Loss: [4.6015849113464355, 0.546875]\n",
      "Epoch: 276 , Loss: [4.649855136871338, 0.484375]\n",
      "Epoch: 277 , Loss: [4.503077507019043, 0.5625]\n",
      "Epoch: 278 , Loss: [4.50034761428833, 0.515625]\n",
      "Epoch: 279 , Loss: [4.440963268280029, 0.46875]\n",
      "Epoch: 280 , Loss: [4.449159145355225, 0.53125]\n",
      "Epoch: 281 , Loss: [4.381717681884766, 0.515625]\n",
      "Epoch: 282 , Loss: [4.336942672729492, 0.46875]\n",
      "Epoch: 283 , Loss: [4.316614151000977, 0.515625]\n",
      "Epoch: 284 , Loss: [4.400974273681641, 0.484375]\n",
      "Epoch: 285 , Loss: [4.278374671936035, 0.4375]\n",
      "Epoch: 286 , Loss: [4.233904838562012, 0.46875]\n",
      "Epoch: 287 , Loss: [4.244334697723389, 0.46875]\n",
      "Epoch: 288 , Loss: [4.368651390075684, 0.375]\n",
      "Epoch: 289 , Loss: [4.145509719848633, 0.515625]\n",
      "Epoch: 290 , Loss: [4.139400959014893, 0.484375]\n",
      "Epoch: 291 , Loss: [4.044862270355225, 0.515625]\n",
      "Epoch: 292 , Loss: [4.153186798095703, 0.453125]\n",
      "Epoch: 293 , Loss: [4.225372791290283, 0.390625]\n",
      "Epoch: 294 , Loss: [4.090907096862793, 0.46875]\n",
      "Epoch: 295 , Loss: [4.011444091796875, 0.484375]\n",
      "Epoch: 296 , Loss: [4.005100727081299, 0.453125]\n",
      "Epoch: 297 , Loss: [3.815361976623535, 0.5625]\n",
      "Epoch: 298 , Loss: [3.9150943756103516, 0.484375]\n",
      "Epoch: 299 , Loss: [3.9992055892944336, 0.453125]\n",
      "Epoch: 300 , Loss: [3.8202579021453857, 0.484375]\n",
      "Epoch: 301 , Loss: [3.7331314086914062, 0.546875]\n",
      "Epoch: 302 , Loss: [3.6668481826782227, 0.578125]\n",
      "Epoch: 303 , Loss: [3.831183910369873, 0.484375]\n",
      "Epoch: 304 , Loss: [3.6643831729888916, 0.484375]\n",
      "Epoch: 305 , Loss: [3.664057731628418, 0.484375]\n",
      "Epoch: 306 , Loss: [3.6155059337615967, 0.46875]\n",
      "Epoch: 307 , Loss: [3.6112217903137207, 0.515625]\n",
      "Epoch: 308 , Loss: [3.622485637664795, 0.5]\n",
      "Epoch: 309 , Loss: [3.532845973968506, 0.546875]\n",
      "Epoch: 310 , Loss: [3.5066187381744385, 0.484375]\n",
      "Epoch: 311 , Loss: [3.6506781578063965, 0.40625]\n",
      "Epoch: 312 , Loss: [3.423252582550049, 0.546875]\n",
      "Epoch: 313 , Loss: [3.4242780208587646, 0.578125]\n",
      "Epoch: 314 , Loss: [3.363752841949463, 0.5]\n",
      "Epoch: 315 , Loss: [3.439899444580078, 0.421875]\n",
      "Epoch: 316 , Loss: [3.4558000564575195, 0.453125]\n",
      "Epoch: 317 , Loss: [3.3154616355895996, 0.546875]\n",
      "Epoch: 318 , Loss: [3.3048629760742188, 0.453125]\n",
      "Epoch: 319 , Loss: [3.292841911315918, 0.515625]\n",
      "Epoch: 320 , Loss: [3.3434486389160156, 0.390625]\n",
      "Epoch: 321 , Loss: [3.34810733795166, 0.46875]\n",
      "Epoch: 322 , Loss: [3.2956488132476807, 0.390625]\n",
      "Epoch: 323 , Loss: [3.151371479034424, 0.59375]\n",
      "Epoch: 324 , Loss: [3.314244270324707, 0.34375]\n",
      "Epoch: 325 , Loss: [3.2318592071533203, 0.453125]\n",
      "Epoch: 326 , Loss: [3.102895736694336, 0.484375]\n",
      "Epoch: 327 , Loss: [3.1201000213623047, 0.5]\n",
      "Epoch: 328 , Loss: [3.0643675327301025, 0.515625]\n",
      "Epoch: 329 , Loss: [3.1545557975769043, 0.546875]\n",
      "Epoch: 330 , Loss: [3.0665814876556396, 0.4375]\n",
      "Epoch: 331 , Loss: [3.04025936126709, 0.46875]\n",
      "Epoch: 332 , Loss: [3.0425968170166016, 0.515625]\n",
      "Epoch: 333 , Loss: [2.8380982875823975, 0.65625]\n",
      "Epoch: 334 , Loss: [2.952549934387207, 0.578125]\n",
      "Epoch: 335 , Loss: [3.040327548980713, 0.421875]\n",
      "Epoch: 336 , Loss: [2.9094786643981934, 0.515625]\n",
      "Epoch: 337 , Loss: [2.8817243576049805, 0.5625]\n",
      "Epoch: 338 , Loss: [2.9082188606262207, 0.578125]\n",
      "Epoch: 339 , Loss: [2.8725368976593018, 0.484375]\n",
      "Epoch: 340 , Loss: [2.8790574073791504, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341 , Loss: [2.8870673179626465, 0.453125]\n",
      "Epoch: 342 , Loss: [2.894804000854492, 0.453125]\n",
      "Epoch: 343 , Loss: [2.8365442752838135, 0.46875]\n",
      "Epoch: 344 , Loss: [2.739819049835205, 0.546875]\n",
      "Epoch: 345 , Loss: [2.7787344455718994, 0.46875]\n",
      "Epoch: 346 , Loss: [2.9192092418670654, 0.375]\n",
      "Epoch: 347 , Loss: [2.713193893432617, 0.578125]\n",
      "Epoch: 348 , Loss: [2.752960681915283, 0.5625]\n",
      "Epoch: 349 , Loss: [2.7465946674346924, 0.453125]\n",
      "Epoch: 350 , Loss: [2.5354130268096924, 0.625]\n",
      "Epoch: 351 , Loss: [2.639453887939453, 0.515625]\n",
      "Epoch: 352 , Loss: [2.6730494499206543, 0.4375]\n",
      "Epoch: 353 , Loss: [2.690403938293457, 0.46875]\n",
      "Epoch: 354 , Loss: [2.770460844039917, 0.40625]\n",
      "Epoch: 355 , Loss: [2.585625410079956, 0.546875]\n",
      "Epoch: 356 , Loss: [2.5912013053894043, 0.515625]\n",
      "Epoch: 357 , Loss: [2.583632230758667, 0.4375]\n",
      "Epoch: 358 , Loss: [2.551177501678467, 0.46875]\n",
      "Epoch: 359 , Loss: [2.6451148986816406, 0.46875]\n",
      "Epoch: 360 , Loss: [2.5640742778778076, 0.40625]\n",
      "Epoch: 361 , Loss: [2.6204590797424316, 0.46875]\n",
      "Epoch: 362 , Loss: [2.425076961517334, 0.546875]\n",
      "Epoch: 363 , Loss: [2.493192434310913, 0.5]\n",
      "Epoch: 364 , Loss: [2.585951805114746, 0.484375]\n",
      "Epoch: 365 , Loss: [2.43772554397583, 0.625]\n",
      "Epoch: 366 , Loss: [2.542848825454712, 0.421875]\n",
      "Epoch: 367 , Loss: [2.3574440479278564, 0.53125]\n",
      "Epoch: 368 , Loss: [2.4200620651245117, 0.484375]\n",
      "Epoch: 369 , Loss: [2.273615837097168, 0.59375]\n",
      "Epoch: 370 , Loss: [2.4099552631378174, 0.5625]\n",
      "Epoch: 371 , Loss: [2.5978262424468994, 0.359375]\n",
      "Epoch: 372 , Loss: [2.305284023284912, 0.640625]\n",
      "Epoch: 373 , Loss: [2.357945203781128, 0.4375]\n",
      "Epoch: 374 , Loss: [2.2622814178466797, 0.59375]\n",
      "Epoch: 375 , Loss: [2.2462961673736572, 0.46875]\n",
      "Epoch: 376 , Loss: [2.1835546493530273, 0.625]\n",
      "Epoch: 377 , Loss: [2.330369472503662, 0.515625]\n",
      "Epoch: 378 , Loss: [2.2757444381713867, 0.484375]\n",
      "Epoch: 379 , Loss: [2.2627854347229004, 0.53125]\n",
      "Epoch: 380 , Loss: [2.179626703262329, 0.59375]\n",
      "Epoch: 381 , Loss: [2.199042558670044, 0.5625]\n",
      "Epoch: 382 , Loss: [2.2112908363342285, 0.5]\n",
      "Epoch: 383 , Loss: [2.205720901489258, 0.546875]\n",
      "Epoch: 384 , Loss: [2.287631034851074, 0.484375]\n",
      "Epoch: 385 , Loss: [2.408175468444824, 0.390625]\n",
      "Epoch: 386 , Loss: [2.2050487995147705, 0.5]\n",
      "Epoch: 387 , Loss: [2.3229012489318848, 0.453125]\n",
      "Epoch: 388 , Loss: [2.171185255050659, 0.515625]\n",
      "Epoch: 389 , Loss: [2.208394765853882, 0.421875]\n",
      "Epoch: 390 , Loss: [2.0859017372131348, 0.578125]\n",
      "Epoch: 391 , Loss: [2.2209362983703613, 0.5]\n",
      "Epoch: 392 , Loss: [2.0600881576538086, 0.5625]\n",
      "Epoch: 393 , Loss: [2.1341981887817383, 0.484375]\n",
      "Epoch: 394 , Loss: [2.1973085403442383, 0.421875]\n",
      "Epoch: 395 , Loss: [2.0057334899902344, 0.5625]\n",
      "Epoch: 396 , Loss: [2.099895477294922, 0.5625]\n",
      "Epoch: 397 , Loss: [2.051257371902466, 0.5625]\n",
      "Epoch: 398 , Loss: [1.885817050933838, 0.640625]\n",
      "Epoch: 399 , Loss: [2.181683301925659, 0.453125]\n",
      "Epoch: 400 , Loss: [2.0488197803497314, 0.4375]\n",
      "Epoch: 401 , Loss: [2.0129013061523438, 0.5]\n",
      "Epoch: 402 , Loss: [2.0354223251342773, 0.53125]\n",
      "Epoch: 403 , Loss: [1.9872392416000366, 0.5]\n",
      "Epoch: 404 , Loss: [2.084580659866333, 0.4375]\n",
      "Epoch: 405 , Loss: [2.177952289581299, 0.328125]\n",
      "Epoch: 406 , Loss: [2.029735565185547, 0.421875]\n",
      "Epoch: 407 , Loss: [1.9845962524414062, 0.53125]\n",
      "Epoch: 408 , Loss: [1.9218717813491821, 0.515625]\n",
      "Epoch: 409 , Loss: [1.8865008354187012, 0.5625]\n",
      "Epoch: 410 , Loss: [1.8473252058029175, 0.59375]\n",
      "Epoch: 411 , Loss: [1.9409507513046265, 0.59375]\n",
      "Epoch: 412 , Loss: [1.9617931842803955, 0.421875]\n",
      "Epoch: 413 , Loss: [1.9334187507629395, 0.484375]\n",
      "Epoch: 414 , Loss: [1.8890149593353271, 0.546875]\n",
      "Epoch: 415 , Loss: [1.9744309186935425, 0.5]\n",
      "Epoch: 416 , Loss: [1.8713140487670898, 0.484375]\n",
      "Epoch: 417 , Loss: [1.800827980041504, 0.5625]\n",
      "Epoch: 418 , Loss: [1.8896512985229492, 0.484375]\n",
      "Epoch: 419 , Loss: [1.9274241924285889, 0.53125]\n",
      "Epoch: 420 , Loss: [1.890798568725586, 0.40625]\n",
      "Epoch: 421 , Loss: [1.8559472560882568, 0.453125]\n",
      "Epoch: 422 , Loss: [1.8589820861816406, 0.46875]\n",
      "Epoch: 423 , Loss: [1.68720543384552, 0.578125]\n",
      "Epoch: 424 , Loss: [1.7902419567108154, 0.53125]\n",
      "Epoch: 425 , Loss: [1.7970893383026123, 0.546875]\n",
      "Epoch: 426 , Loss: [1.7089036703109741, 0.546875]\n",
      "Epoch: 427 , Loss: [1.797621726989746, 0.484375]\n",
      "Epoch: 428 , Loss: [1.815029501914978, 0.421875]\n",
      "Epoch: 429 , Loss: [1.8682698011398315, 0.421875]\n",
      "Epoch: 430 , Loss: [1.8315519094467163, 0.40625]\n",
      "Epoch: 431 , Loss: [1.7643256187438965, 0.484375]\n",
      "Epoch: 432 , Loss: [1.7745883464813232, 0.515625]\n",
      "Epoch: 433 , Loss: [1.7473294734954834, 0.453125]\n",
      "Epoch: 434 , Loss: [1.8823659420013428, 0.375]\n",
      "Epoch: 435 , Loss: [1.752198338508606, 0.484375]\n",
      "Epoch: 436 , Loss: [1.7841103076934814, 0.421875]\n",
      "Epoch: 437 , Loss: [1.713174819946289, 0.546875]\n",
      "Epoch: 438 , Loss: [1.6952149868011475, 0.59375]\n",
      "Epoch: 439 , Loss: [1.8393465280532837, 0.359375]\n",
      "Epoch: 440 , Loss: [1.792493462562561, 0.453125]\n",
      "Epoch: 441 , Loss: [1.676558017730713, 0.546875]\n",
      "Epoch: 442 , Loss: [1.6944166421890259, 0.53125]\n",
      "Epoch: 443 , Loss: [1.8615429401397705, 0.296875]\n",
      "Epoch: 444 , Loss: [1.7609795331954956, 0.453125]\n",
      "Epoch: 445 , Loss: [1.6542556285858154, 0.484375]\n",
      "Epoch: 446 , Loss: [1.6611850261688232, 0.484375]\n",
      "Epoch: 447 , Loss: [1.6949816942214966, 0.40625]\n",
      "Epoch: 448 , Loss: [1.6377919912338257, 0.484375]\n",
      "Epoch: 449 , Loss: [1.6053985357284546, 0.5625]\n",
      "Epoch: 450 , Loss: [1.645282506942749, 0.4375]\n",
      "Epoch: 451 , Loss: [1.5638434886932373, 0.515625]\n",
      "Epoch: 452 , Loss: [1.4563473463058472, 0.703125]\n",
      "Epoch: 453 , Loss: [1.6220331192016602, 0.4375]\n",
      "Epoch: 454 , Loss: [1.635471224784851, 0.421875]\n",
      "Epoch: 455 , Loss: [1.6512961387634277, 0.484375]\n",
      "Epoch: 456 , Loss: [1.594296932220459, 0.515625]\n",
      "Epoch: 457 , Loss: [1.5318036079406738, 0.515625]\n",
      "Epoch: 458 , Loss: [1.63826322555542, 0.421875]\n",
      "Epoch: 459 , Loss: [1.5900360345840454, 0.484375]\n",
      "Epoch: 460 , Loss: [1.595299243927002, 0.46875]\n",
      "Epoch: 461 , Loss: [1.5801326036453247, 0.53125]\n",
      "Epoch: 462 , Loss: [1.5297902822494507, 0.4375]\n",
      "Epoch: 463 , Loss: [1.4947359561920166, 0.625]\n",
      "Epoch: 464 , Loss: [1.5508817434310913, 0.484375]\n",
      "Epoch: 465 , Loss: [1.5501004457473755, 0.484375]\n",
      "Epoch: 466 , Loss: [1.511345386505127, 0.515625]\n",
      "Epoch: 467 , Loss: [1.5980013608932495, 0.359375]\n",
      "Epoch: 468 , Loss: [1.49220609664917, 0.5]\n",
      "Epoch: 469 , Loss: [1.498429775238037, 0.4375]\n",
      "Epoch: 470 , Loss: [1.493443489074707, 0.453125]\n",
      "Epoch: 471 , Loss: [1.4871447086334229, 0.53125]\n",
      "Epoch: 472 , Loss: [1.3585093021392822, 0.640625]\n",
      "Epoch: 473 , Loss: [1.491464376449585, 0.46875]\n",
      "Epoch: 474 , Loss: [1.5136747360229492, 0.484375]\n",
      "Epoch: 475 , Loss: [1.5718269348144531, 0.4375]\n",
      "Epoch: 476 , Loss: [1.3986188173294067, 0.578125]\n",
      "Epoch: 477 , Loss: [1.437349557876587, 0.5625]\n",
      "Epoch: 478 , Loss: [1.435417652130127, 0.515625]\n",
      "Epoch: 479 , Loss: [1.4200925827026367, 0.578125]\n",
      "Epoch: 480 , Loss: [1.4581832885742188, 0.515625]\n",
      "Epoch: 481 , Loss: [1.4207768440246582, 0.625]\n",
      "Epoch: 482 , Loss: [1.4686970710754395, 0.421875]\n",
      "Epoch: 483 , Loss: [1.4318395853042603, 0.421875]\n",
      "Epoch: 484 , Loss: [1.4680938720703125, 0.40625]\n",
      "Epoch: 485 , Loss: [1.4297584295272827, 0.53125]\n",
      "Epoch: 486 , Loss: [1.3996505737304688, 0.5]\n",
      "Epoch: 487 , Loss: [1.4725046157836914, 0.46875]\n",
      "Epoch: 488 , Loss: [1.347616195678711, 0.578125]\n",
      "Epoch: 489 , Loss: [1.4140474796295166, 0.5625]\n",
      "Epoch: 490 , Loss: [1.3680940866470337, 0.5625]\n",
      "Epoch: 491 , Loss: [1.3480067253112793, 0.578125]\n",
      "Epoch: 492 , Loss: [1.4179069995880127, 0.5625]\n",
      "Epoch: 493 , Loss: [1.4057669639587402, 0.453125]\n",
      "Epoch: 494 , Loss: [1.5282106399536133, 0.390625]\n",
      "Epoch: 495 , Loss: [1.3459668159484863, 0.546875]\n",
      "Epoch: 496 , Loss: [1.3879897594451904, 0.53125]\n",
      "Epoch: 497 , Loss: [1.3907086849212646, 0.453125]\n",
      "Epoch: 498 , Loss: [1.3580456972122192, 0.4375]\n",
      "Epoch: 499 , Loss: [1.388624906539917, 0.53125]\n",
      "Epoch: 500 , Loss: [1.460798978805542, 0.40625]\n",
      "=============================================\n",
      "1 correctly classified among 64\n",
      "Accuracy as of 500 epochs: 1.5625\n",
      "=============================================\n",
      "Epoch: 501 , Loss: [1.3011597394943237, 0.59375]\n",
      "Epoch: 502 , Loss: [1.3316811323165894, 0.484375]\n",
      "Epoch: 503 , Loss: [1.4832775592803955, 0.359375]\n",
      "Epoch: 504 , Loss: [1.3598567247390747, 0.546875]\n",
      "Epoch: 505 , Loss: [1.2384438514709473, 0.640625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 506 , Loss: [1.3402433395385742, 0.515625]\n",
      "Epoch: 507 , Loss: [1.3389471769332886, 0.421875]\n",
      "Epoch: 508 , Loss: [1.3057879209518433, 0.5]\n",
      "Epoch: 509 , Loss: [1.3975839614868164, 0.4375]\n",
      "Epoch: 510 , Loss: [1.29679274559021, 0.578125]\n",
      "Epoch: 511 , Loss: [1.2843254804611206, 0.53125]\n",
      "Epoch: 512 , Loss: [1.275181531906128, 0.546875]\n",
      "Epoch: 513 , Loss: [1.311950922012329, 0.515625]\n",
      "Epoch: 514 , Loss: [1.2736895084381104, 0.515625]\n",
      "Epoch: 515 , Loss: [1.2668123245239258, 0.59375]\n",
      "Epoch: 516 , Loss: [1.3295429944992065, 0.46875]\n",
      "Epoch: 517 , Loss: [1.3528109788894653, 0.421875]\n",
      "Epoch: 518 , Loss: [1.2747678756713867, 0.53125]\n",
      "Epoch: 519 , Loss: [1.220332384109497, 0.59375]\n",
      "Epoch: 520 , Loss: [1.2520220279693604, 0.59375]\n",
      "Epoch: 521 , Loss: [1.2588272094726562, 0.546875]\n",
      "Epoch: 522 , Loss: [1.2681409120559692, 0.5625]\n",
      "Epoch: 523 , Loss: [1.282322883605957, 0.484375]\n",
      "Epoch: 524 , Loss: [1.2822538614273071, 0.453125]\n",
      "Epoch: 525 , Loss: [1.2370988130569458, 0.53125]\n",
      "Epoch: 526 , Loss: [1.3109499216079712, 0.453125]\n",
      "Epoch: 527 , Loss: [1.2280887365341187, 0.59375]\n",
      "Epoch: 528 , Loss: [1.3113523721694946, 0.5]\n",
      "Epoch: 529 , Loss: [1.33345365524292, 0.421875]\n",
      "Epoch: 530 , Loss: [1.2505393028259277, 0.515625]\n",
      "Epoch: 531 , Loss: [1.3125473260879517, 0.453125]\n",
      "Epoch: 532 , Loss: [1.3192533254623413, 0.484375]\n",
      "Epoch: 533 , Loss: [1.2417105436325073, 0.5625]\n",
      "Epoch: 534 , Loss: [1.34092378616333, 0.484375]\n",
      "Epoch: 535 , Loss: [1.4222434759140015, 0.328125]\n",
      "Epoch: 536 , Loss: [1.1990638971328735, 0.53125]\n",
      "Epoch: 537 , Loss: [1.1878935098648071, 0.53125]\n",
      "Epoch: 538 , Loss: [1.3366804122924805, 0.40625]\n",
      "Epoch: 539 , Loss: [1.1925632953643799, 0.546875]\n",
      "Epoch: 540 , Loss: [1.1913607120513916, 0.609375]\n",
      "Epoch: 541 , Loss: [1.1747117042541504, 0.578125]\n",
      "Epoch: 542 , Loss: [1.196191430091858, 0.5]\n",
      "Epoch: 543 , Loss: [1.2449531555175781, 0.5]\n",
      "Epoch: 544 , Loss: [1.1551908254623413, 0.578125]\n",
      "Epoch: 545 , Loss: [1.1587201356887817, 0.53125]\n",
      "Epoch: 546 , Loss: [1.1918487548828125, 0.609375]\n",
      "Epoch: 547 , Loss: [1.2342417240142822, 0.484375]\n",
      "Epoch: 548 , Loss: [1.2068219184875488, 0.46875]\n",
      "Epoch: 549 , Loss: [1.2169342041015625, 0.515625]\n",
      "Epoch: 550 , Loss: [1.1827958822250366, 0.53125]\n",
      "Epoch: 551 , Loss: [1.0982569456100464, 0.546875]\n",
      "Epoch: 552 , Loss: [1.2077710628509521, 0.5]\n",
      "Epoch: 553 , Loss: [1.157759666442871, 0.53125]\n",
      "Epoch: 554 , Loss: [1.2929582595825195, 0.453125]\n",
      "Epoch: 555 , Loss: [1.2628215551376343, 0.421875]\n",
      "Epoch: 556 , Loss: [1.1541414260864258, 0.53125]\n",
      "Epoch: 557 , Loss: [1.2165765762329102, 0.46875]\n",
      "Epoch: 558 , Loss: [1.155348777770996, 0.515625]\n",
      "Epoch: 559 , Loss: [1.1500327587127686, 0.5]\n",
      "Epoch: 560 , Loss: [1.1416019201278687, 0.578125]\n",
      "Epoch: 561 , Loss: [1.1646082401275635, 0.5]\n",
      "Epoch: 562 , Loss: [1.2403171062469482, 0.46875]\n",
      "Epoch: 563 , Loss: [1.097605586051941, 0.546875]\n",
      "Epoch: 564 , Loss: [1.1328202486038208, 0.546875]\n",
      "Epoch: 565 , Loss: [1.231022834777832, 0.46875]\n",
      "Epoch: 566 , Loss: [1.1491608619689941, 0.46875]\n",
      "Epoch: 567 , Loss: [1.1391927003860474, 0.515625]\n",
      "Epoch: 568 , Loss: [1.1566702127456665, 0.53125]\n",
      "Epoch: 569 , Loss: [1.1806387901306152, 0.515625]\n",
      "Epoch: 570 , Loss: [1.1246412992477417, 0.546875]\n",
      "Epoch: 571 , Loss: [1.1474616527557373, 0.546875]\n",
      "Epoch: 572 , Loss: [1.1472736597061157, 0.46875]\n",
      "Epoch: 573 , Loss: [1.1254539489746094, 0.5]\n",
      "Epoch: 574 , Loss: [1.1798912286758423, 0.484375]\n",
      "Epoch: 575 , Loss: [1.0747454166412354, 0.53125]\n",
      "Epoch: 576 , Loss: [1.2044099569320679, 0.578125]\n",
      "Epoch: 577 , Loss: [1.163172721862793, 0.390625]\n",
      "Epoch: 578 , Loss: [1.159535527229309, 0.453125]\n",
      "Epoch: 579 , Loss: [1.1278914213180542, 0.515625]\n",
      "Epoch: 580 , Loss: [1.1073920726776123, 0.578125]\n",
      "Epoch: 581 , Loss: [1.1284773349761963, 0.53125]\n",
      "Epoch: 582 , Loss: [1.2431974411010742, 0.46875]\n",
      "Epoch: 583 , Loss: [1.148057460784912, 0.4375]\n",
      "Epoch: 584 , Loss: [1.1770672798156738, 0.5]\n",
      "Epoch: 585 , Loss: [1.1596845388412476, 0.421875]\n",
      "Epoch: 586 , Loss: [1.0834437608718872, 0.421875]\n",
      "Epoch: 587 , Loss: [1.0532472133636475, 0.515625]\n",
      "Epoch: 588 , Loss: [1.1399133205413818, 0.4375]\n",
      "Epoch: 589 , Loss: [1.1302493810653687, 0.46875]\n",
      "Epoch: 590 , Loss: [1.0977519750595093, 0.5625]\n",
      "Epoch: 591 , Loss: [1.0325199365615845, 0.578125]\n",
      "Epoch: 592 , Loss: [1.1619007587432861, 0.515625]\n",
      "Epoch: 593 , Loss: [1.1708521842956543, 0.421875]\n",
      "Epoch: 594 , Loss: [1.2514989376068115, 0.46875]\n",
      "Epoch: 595 , Loss: [1.1452960968017578, 0.4375]\n",
      "Epoch: 596 , Loss: [1.1364750862121582, 0.515625]\n",
      "Epoch: 597 , Loss: [1.0831356048583984, 0.46875]\n",
      "Epoch: 598 , Loss: [1.1430559158325195, 0.40625]\n",
      "Epoch: 599 , Loss: [1.0767090320587158, 0.53125]\n",
      "Epoch: 600 , Loss: [1.065004825592041, 0.5]\n",
      "Epoch: 601 , Loss: [1.1409300565719604, 0.484375]\n",
      "Epoch: 602 , Loss: [1.0135471820831299, 0.515625]\n",
      "Epoch: 603 , Loss: [1.0018900632858276, 0.59375]\n",
      "Epoch: 604 , Loss: [1.0969829559326172, 0.5]\n",
      "Epoch: 605 , Loss: [1.043643832206726, 0.515625]\n",
      "Epoch: 606 , Loss: [1.0094749927520752, 0.640625]\n",
      "Epoch: 607 , Loss: [1.1053760051727295, 0.421875]\n",
      "Epoch: 608 , Loss: [1.1296273469924927, 0.5]\n",
      "Epoch: 609 , Loss: [1.0225106477737427, 0.578125]\n",
      "Epoch: 610 , Loss: [1.0912652015686035, 0.453125]\n",
      "Epoch: 611 , Loss: [0.9821491241455078, 0.515625]\n",
      "Epoch: 612 , Loss: [1.1141633987426758, 0.390625]\n",
      "Epoch: 613 , Loss: [1.0355448722839355, 0.515625]\n",
      "Epoch: 614 , Loss: [1.0705583095550537, 0.5]\n",
      "Epoch: 615 , Loss: [1.1348077058792114, 0.46875]\n",
      "Epoch: 616 , Loss: [1.0992289781570435, 0.4375]\n",
      "Epoch: 617 , Loss: [1.02589750289917, 0.5625]\n",
      "Epoch: 618 , Loss: [0.9804985523223877, 0.578125]\n",
      "Epoch: 619 , Loss: [1.0788019895553589, 0.4375]\n",
      "Epoch: 620 , Loss: [1.0045779943466187, 0.5]\n",
      "Epoch: 621 , Loss: [0.9988958835601807, 0.578125]\n",
      "Epoch: 622 , Loss: [1.048506498336792, 0.484375]\n",
      "Epoch: 623 , Loss: [0.9419337511062622, 0.609375]\n",
      "Epoch: 624 , Loss: [1.130252480506897, 0.421875]\n",
      "Epoch: 625 , Loss: [1.1663645505905151, 0.390625]\n",
      "Epoch: 626 , Loss: [1.0699808597564697, 0.453125]\n",
      "Epoch: 627 , Loss: [0.9131488800048828, 0.671875]\n",
      "Epoch: 628 , Loss: [1.1296749114990234, 0.40625]\n",
      "Epoch: 629 , Loss: [0.9463381767272949, 0.578125]\n",
      "Epoch: 630 , Loss: [1.082719087600708, 0.4375]\n",
      "Epoch: 631 , Loss: [1.049726963043213, 0.5]\n",
      "Epoch: 632 , Loss: [1.0271527767181396, 0.53125]\n",
      "Epoch: 633 , Loss: [1.042771577835083, 0.453125]\n",
      "Epoch: 634 , Loss: [0.9692835807800293, 0.5625]\n",
      "Epoch: 635 , Loss: [0.910184383392334, 0.65625]\n",
      "Epoch: 636 , Loss: [1.113577961921692, 0.421875]\n",
      "Epoch: 637 , Loss: [1.019749641418457, 0.5]\n",
      "Epoch: 638 , Loss: [1.1050714254379272, 0.515625]\n",
      "Epoch: 639 , Loss: [1.0989843606948853, 0.421875]\n",
      "Epoch: 640 , Loss: [1.0486738681793213, 0.46875]\n",
      "Epoch: 641 , Loss: [0.9879671931266785, 0.5625]\n",
      "Epoch: 642 , Loss: [1.0507128238677979, 0.46875]\n",
      "Epoch: 643 , Loss: [1.0231834650039673, 0.5]\n",
      "Epoch: 644 , Loss: [0.9368254542350769, 0.640625]\n",
      "Epoch: 645 , Loss: [1.0079513788223267, 0.46875]\n",
      "Epoch: 646 , Loss: [1.0002447366714478, 0.46875]\n",
      "Epoch: 647 , Loss: [1.0984090566635132, 0.4375]\n",
      "Epoch: 648 , Loss: [0.922310471534729, 0.640625]\n",
      "Epoch: 649 , Loss: [1.0165202617645264, 0.515625]\n",
      "Epoch: 650 , Loss: [0.9737499952316284, 0.53125]\n",
      "Epoch: 651 , Loss: [0.9407965540885925, 0.609375]\n",
      "Epoch: 652 , Loss: [1.0588213205337524, 0.453125]\n",
      "Epoch: 653 , Loss: [1.0528603792190552, 0.453125]\n",
      "Epoch: 654 , Loss: [0.9846630692481995, 0.453125]\n",
      "Epoch: 655 , Loss: [1.0168815851211548, 0.515625]\n",
      "Epoch: 656 , Loss: [1.0608466863632202, 0.46875]\n",
      "Epoch: 657 , Loss: [0.9423817992210388, 0.578125]\n",
      "Epoch: 658 , Loss: [1.0605311393737793, 0.453125]\n",
      "Epoch: 659 , Loss: [0.951096773147583, 0.5]\n",
      "Epoch: 660 , Loss: [0.9444594383239746, 0.546875]\n",
      "Epoch: 661 , Loss: [1.0780091285705566, 0.5]\n",
      "Epoch: 662 , Loss: [0.9927110075950623, 0.5]\n",
      "Epoch: 663 , Loss: [1.0181236267089844, 0.546875]\n",
      "Epoch: 664 , Loss: [1.0857822895050049, 0.375]\n",
      "Epoch: 665 , Loss: [0.9587116241455078, 0.53125]\n",
      "Epoch: 666 , Loss: [0.9499457478523254, 0.53125]\n",
      "Epoch: 667 , Loss: [0.9685333371162415, 0.5]\n",
      "Epoch: 668 , Loss: [1.008394718170166, 0.421875]\n",
      "Epoch: 669 , Loss: [0.9831340312957764, 0.53125]\n",
      "Epoch: 670 , Loss: [1.013660192489624, 0.484375]\n",
      "Epoch: 671 , Loss: [0.9491685032844543, 0.484375]\n",
      "Epoch: 672 , Loss: [1.0175304412841797, 0.4375]\n",
      "Epoch: 673 , Loss: [1.0076868534088135, 0.421875]\n",
      "Epoch: 674 , Loss: [0.9857439994812012, 0.546875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 675 , Loss: [0.9948355555534363, 0.453125]\n",
      "Epoch: 676 , Loss: [1.0486313104629517, 0.46875]\n",
      "Epoch: 677 , Loss: [0.9586559534072876, 0.515625]\n",
      "Epoch: 678 , Loss: [0.9955691695213318, 0.515625]\n",
      "Epoch: 679 , Loss: [0.9542880654335022, 0.578125]\n",
      "Epoch: 680 , Loss: [0.9854270219802856, 0.484375]\n",
      "Epoch: 681 , Loss: [0.9558144807815552, 0.453125]\n",
      "Epoch: 682 , Loss: [0.9905003905296326, 0.53125]\n",
      "Epoch: 683 , Loss: [0.9385541677474976, 0.546875]\n",
      "Epoch: 684 , Loss: [0.9595223069190979, 0.515625]\n",
      "Epoch: 685 , Loss: [1.0013117790222168, 0.484375]\n",
      "Epoch: 686 , Loss: [0.9227609634399414, 0.578125]\n",
      "Epoch: 687 , Loss: [0.9269557595252991, 0.53125]\n",
      "Epoch: 688 , Loss: [0.928653359413147, 0.53125]\n",
      "Epoch: 689 , Loss: [0.9856292009353638, 0.453125]\n",
      "Epoch: 690 , Loss: [0.9418770670890808, 0.5625]\n",
      "Epoch: 691 , Loss: [0.9918065071105957, 0.4375]\n",
      "Epoch: 692 , Loss: [0.8612917065620422, 0.578125]\n",
      "Epoch: 693 , Loss: [0.9926767349243164, 0.4375]\n",
      "Epoch: 694 , Loss: [0.9582979679107666, 0.40625]\n",
      "Epoch: 695 , Loss: [0.8519175052642822, 0.609375]\n",
      "Epoch: 696 , Loss: [0.9699603915214539, 0.453125]\n",
      "Epoch: 697 , Loss: [0.8889495730400085, 0.609375]\n",
      "Epoch: 698 , Loss: [0.9353268146514893, 0.546875]\n",
      "Epoch: 699 , Loss: [0.9384818077087402, 0.546875]\n",
      "Epoch: 700 , Loss: [0.9499384164810181, 0.421875]\n",
      "Epoch: 701 , Loss: [0.8945754170417786, 0.609375]\n",
      "Epoch: 702 , Loss: [0.9335544109344482, 0.5]\n",
      "Epoch: 703 , Loss: [0.9633262157440186, 0.515625]\n",
      "Epoch: 704 , Loss: [0.9413537979125977, 0.515625]\n",
      "Epoch: 705 , Loss: [0.9390485286712646, 0.5625]\n",
      "Epoch: 706 , Loss: [1.0299153327941895, 0.359375]\n",
      "Epoch: 707 , Loss: [1.0080265998840332, 0.46875]\n",
      "Epoch: 708 , Loss: [0.9375824332237244, 0.5]\n",
      "Epoch: 709 , Loss: [0.9113601446151733, 0.546875]\n",
      "Epoch: 710 , Loss: [0.9328357577323914, 0.59375]\n",
      "Epoch: 711 , Loss: [0.9810836315155029, 0.484375]\n",
      "Epoch: 712 , Loss: [0.9828560948371887, 0.515625]\n",
      "Epoch: 713 , Loss: [0.9219747185707092, 0.53125]\n",
      "Epoch: 714 , Loss: [0.9905592203140259, 0.46875]\n",
      "Epoch: 715 , Loss: [0.894931435585022, 0.5625]\n",
      "Epoch: 716 , Loss: [0.9416415095329285, 0.5]\n",
      "Epoch: 717 , Loss: [1.0468621253967285, 0.375]\n",
      "Epoch: 718 , Loss: [0.9505103230476379, 0.46875]\n",
      "Epoch: 719 , Loss: [0.8954458832740784, 0.546875]\n",
      "Epoch: 720 , Loss: [1.0202360153198242, 0.359375]\n",
      "Epoch: 721 , Loss: [0.9653075933456421, 0.46875]\n",
      "Epoch: 722 , Loss: [0.9954173564910889, 0.4375]\n",
      "Epoch: 723 , Loss: [0.9202970862388611, 0.5]\n",
      "Epoch: 724 , Loss: [0.8594303131103516, 0.609375]\n",
      "Epoch: 725 , Loss: [0.8837071061134338, 0.578125]\n",
      "Epoch: 726 , Loss: [0.9480164051055908, 0.46875]\n",
      "Epoch: 727 , Loss: [0.8916710019111633, 0.578125]\n",
      "Epoch: 728 , Loss: [0.9501191973686218, 0.5]\n",
      "Epoch: 729 , Loss: [0.9388216733932495, 0.515625]\n",
      "Epoch: 730 , Loss: [0.9977232217788696, 0.4375]\n",
      "Epoch: 731 , Loss: [0.8985375165939331, 0.484375]\n",
      "Epoch: 732 , Loss: [0.8986926078796387, 0.53125]\n",
      "Epoch: 733 , Loss: [0.8853999376296997, 0.515625]\n",
      "Epoch: 734 , Loss: [0.9293062090873718, 0.5]\n",
      "Epoch: 735 , Loss: [1.0250189304351807, 0.375]\n",
      "Epoch: 736 , Loss: [0.8808062076568604, 0.625]\n",
      "Epoch: 737 , Loss: [0.9375544786453247, 0.46875]\n",
      "Epoch: 738 , Loss: [0.933114767074585, 0.46875]\n",
      "Epoch: 739 , Loss: [0.940562903881073, 0.515625]\n",
      "Epoch: 740 , Loss: [0.8220990896224976, 0.578125]\n",
      "Epoch: 741 , Loss: [0.9130147099494934, 0.421875]\n",
      "Epoch: 742 , Loss: [0.9573084115982056, 0.46875]\n",
      "Epoch: 743 , Loss: [0.8850699663162231, 0.5625]\n",
      "Epoch: 744 , Loss: [0.986688494682312, 0.453125]\n",
      "Epoch: 745 , Loss: [0.8753160238265991, 0.53125]\n",
      "Epoch: 746 , Loss: [0.9021671414375305, 0.390625]\n",
      "Epoch: 747 , Loss: [0.8840228915214539, 0.46875]\n",
      "Epoch: 748 , Loss: [0.925622820854187, 0.421875]\n",
      "Epoch: 749 , Loss: [0.9618787169456482, 0.453125]\n",
      "Epoch: 750 , Loss: [0.9782111644744873, 0.390625]\n",
      "=============================================\n",
      "2 correctly classified among 64\n",
      "Accuracy as of 750 epochs: 3.125\n",
      "=============================================\n",
      "Epoch: 751 , Loss: [0.8674513697624207, 0.5625]\n",
      "Epoch: 752 , Loss: [0.9573974013328552, 0.46875]\n",
      "Epoch: 753 , Loss: [0.9529266953468323, 0.453125]\n",
      "Epoch: 754 , Loss: [0.8756921291351318, 0.53125]\n",
      "Epoch: 755 , Loss: [0.913850724697113, 0.5]\n",
      "Epoch: 756 , Loss: [0.8325878977775574, 0.578125]\n",
      "Epoch: 757 , Loss: [0.876129150390625, 0.546875]\n",
      "Epoch: 758 , Loss: [0.8979589343070984, 0.515625]\n",
      "Epoch: 759 , Loss: [0.9583863019943237, 0.4375]\n",
      "Epoch: 760 , Loss: [0.9033269882202148, 0.5]\n",
      "Epoch: 761 , Loss: [0.9479917287826538, 0.5]\n",
      "Epoch: 762 , Loss: [0.8716842532157898, 0.578125]\n",
      "Epoch: 763 , Loss: [0.8916922807693481, 0.53125]\n",
      "Epoch: 764 , Loss: [0.9518322944641113, 0.46875]\n",
      "Epoch: 765 , Loss: [0.9822827577590942, 0.53125]\n",
      "Epoch: 766 , Loss: [0.8853961825370789, 0.53125]\n",
      "Epoch: 767 , Loss: [0.8931886553764343, 0.421875]\n",
      "Epoch: 768 , Loss: [0.9434273838996887, 0.46875]\n",
      "Epoch: 769 , Loss: [0.8819642066955566, 0.46875]\n",
      "Epoch: 770 , Loss: [0.9557680487632751, 0.484375]\n",
      "Epoch: 771 , Loss: [0.9247625470161438, 0.515625]\n",
      "Epoch: 772 , Loss: [0.9146973490715027, 0.484375]\n",
      "Epoch: 773 , Loss: [0.9203037619590759, 0.484375]\n",
      "Epoch: 774 , Loss: [0.8218315839767456, 0.609375]\n",
      "Epoch: 775 , Loss: [0.8946012258529663, 0.515625]\n",
      "Epoch: 776 , Loss: [0.905912458896637, 0.515625]\n",
      "Epoch: 777 , Loss: [0.8644043803215027, 0.5]\n",
      "Epoch: 778 , Loss: [0.9574810266494751, 0.46875]\n",
      "Epoch: 779 , Loss: [0.9103869795799255, 0.46875]\n",
      "Epoch: 780 , Loss: [0.9026346802711487, 0.484375]\n",
      "Epoch: 781 , Loss: [0.8480890393257141, 0.515625]\n",
      "Epoch: 782 , Loss: [0.8823993802070618, 0.453125]\n",
      "Epoch: 783 , Loss: [0.9133729338645935, 0.46875]\n",
      "Epoch: 784 , Loss: [0.8749105334281921, 0.484375]\n",
      "Epoch: 785 , Loss: [0.8816061019897461, 0.53125]\n",
      "Epoch: 786 , Loss: [0.8341009020805359, 0.53125]\n",
      "Epoch: 787 , Loss: [1.0128989219665527, 0.328125]\n",
      "Epoch: 788 , Loss: [0.9141035676002502, 0.359375]\n",
      "Epoch: 789 , Loss: [0.9041731357574463, 0.40625]\n",
      "Epoch: 790 , Loss: [0.864290714263916, 0.59375]\n",
      "Epoch: 791 , Loss: [0.8366260528564453, 0.546875]\n",
      "Epoch: 792 , Loss: [0.8987743258476257, 0.53125]\n",
      "Epoch: 793 , Loss: [0.8736371397972107, 0.4375]\n",
      "Epoch: 794 , Loss: [0.8680707812309265, 0.53125]\n",
      "Epoch: 795 , Loss: [0.8755921125411987, 0.46875]\n",
      "Epoch: 796 , Loss: [0.8888844847679138, 0.46875]\n",
      "Epoch: 797 , Loss: [0.8711826205253601, 0.5]\n",
      "Epoch: 798 , Loss: [0.889617919921875, 0.484375]\n",
      "Epoch: 799 , Loss: [0.8549098372459412, 0.5]\n",
      "Epoch: 800 , Loss: [0.798155665397644, 0.5625]\n",
      "Epoch: 801 , Loss: [0.8671215176582336, 0.453125]\n",
      "Epoch: 802 , Loss: [0.8637029528617859, 0.5]\n",
      "Epoch: 803 , Loss: [0.9087569713592529, 0.421875]\n",
      "Epoch: 804 , Loss: [0.8570824265480042, 0.46875]\n",
      "Epoch: 805 , Loss: [0.8541815876960754, 0.5]\n",
      "Epoch: 806 , Loss: [0.871260404586792, 0.625]\n",
      "Epoch: 807 , Loss: [1.0071601867675781, 0.328125]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-379f979404c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', Loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                                     class_weight)\n\u001b[1;32m   1694\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Face-Detection/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "n_way = 20\n",
    "n_val = 64\n",
    "batch_size = 64\n",
    "\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "for epoch in range(1,epochs):\n",
    "    batch_x, batch_y = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(batch_x, batch_y)\n",
    "    loss_list.append((epoch,loss))\n",
    "    print('Epoch:', epoch, ', Loss:',loss)\n",
    "    if epoch%250 == 0:\n",
    "        print(\"=============================================\")\n",
    "        accuracy = one_shot_learning(model, n_way, n_val)\n",
    "        accuracy_list.append((epoch, accuracy))\n",
    "        print('Accuracy as of', epoch, 'epochs:', accuracy)\n",
    "        print(\"=============================================\")\n",
    "        if(accuracy>99):\n",
    "            print(\"Achieved more than 90% Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Face-Detection] *",
   "language": "python",
   "name": "conda-env-Face-Detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
